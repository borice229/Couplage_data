---
title: "Machine Learning et ADC"
author: "Borice DOSSOU"
date: "2024-09-04"
output:
  html_document: default
  pdf_document: default
---

```{r}
rm(list = ls())
```

# Chargement des library
```{r}
library(ggplot2)
library(tidyr)
library(tidyverse)
library(reshape2)
library(dplyr)
library(DBI)
library(RMySQL)
library(jsonlite)
library(nloptr)
library(nnet)
```


Fonction generatrice de la suite
```{r}
# Définition de la fonction
Generate_output <- function(N, a, b, c, U0) {
  # Initialisation U0
  U <- numeric(N + 1)
  U[1] <- U0
  
  # Boucle pour calculer les valeurs de U_n
  for (n in 1:N) {
    U[n + 1] <- a * U[n]^c + b
  }
  
  return(U)
}
```


La fonction de test de function
_Essai 1_ 

```{r}
test_Generate_output <- function(){
 a <- 2
 b <- 2
 c <- 1
 U0 <- 1
 N <- 2
 waited<-c(2,2,2,1,1,4,10)
 fun_out<-c(N,a,b,c,Generate_output(N,a,b,c,U0))
 if(all(waited==fun_out)){
   return("OK")
 }else{
   return("Error...")
   }
}
test_Generate_output()
```

__Essai 2__

```{r}
test_Generate_output <- function(){
 a <- 2
 b <- 2
 c <- 1
 U0 <- 1
 N <- 1
 waited<-c(1,2,2,1,1,4)
 fun_out<-c(N,a,b,c,Generate_output(N,a,b,c,U0))
 if(all(waited==fun_out)){
   return("OK")
 }else{
   return("Error...")
   }
}
test_Generate_output()
```
Après avoir Exécuter le code de la fonction. Je constate que le code marche alors je peux le deployer.

Generer les courbes selons les valeurs des paramètres
Let N = 40, a ∈ [2; 5], b ∈ [1; 5] and , c ∈ [0.1; 0.9]

```{r}
# Pour rendre les résultats reproductibles (facultatif)
# Nombre d'itérations
N <- 40
U0 <- 2  # Valeur initiale de U(0)

# Paramètres pour la courbe 1
a1<-2
b1<-1
c1<-0.1
# Paramètres pour la courbe 2
a2<-5
b2<-5
c2<-0.9
# Paramètres pour la courbe 3
a3<-3.5
b3<-3
c3<-0.5
# Paramètres pour la courbe 4
a4 <- 4
b4 <- 2
c4 <- 0.3
# Paramètres pour la courbe 5
a5 <- 2.5
b5 <- 4
c5 <- 0.7
# Paramètres pour la courbe 6
a6 <- 3
b6 <- 1.5
c6 <- 0.4

# Pour permettre un affichage en grille des graphique
par(mfrow = c(3, 2))
# Générer et tracer la courbe 1
plot(0:N, Generate_output(N, a1,b1,c1, U0),
     type = "o", col = 1, ylim = c(0, 4), 
     xlab = "n", ylab = "U_n", 
     main = "Courbe de a=2, b=1, c=0.1")
# Générer et tracer la courbe 2
plot(0:N, Generate_output(N, a2,b2,c2, U0),
     type = "o", col = 1, ylim = c(0, 10^7), 
     xlab = "n", ylab = "U_n", 
     main = "Courbe de a=5, b=5, c=0.9")
# Générer et tracer la courbe 3
plot(0:N, Generate_output(N, a3,b3,c3, U0),
     type = "o", col = 1, ylim = c(0, 20), 
     xlab = "n", ylab = "U_n", 
     main = "Courbe de a=3.5, b=3, c=0.5")
# Générer et tracer la courbe 4
plot(0:N, Generate_output(N, a4, b4, c4, U0),
     type = "o", col = 1, ylim = c(0, 15), 
     xlab = "n", ylab = "U_n", 
     main = "Courbe de a=4, b=2, c=0.3")

# Générer et tracer la courbe 5
plot(0:N, Generate_output(N, a5, b5, c5, U0),
     type = "o", col = 1, ylim = c(0, 40), 
     xlab = "n", ylab = "U_n", 
     main = "Courbe de a=2.5, b=4, c=0.7")

# Générer et tracer la courbe 6
plot(0:N, Generate_output(N, a6, b6, c6, U0),
     type = "o", col = 1, ylim = c(0, 10), 
     xlab = "n", ylab = "U_n", 
     main = "Courbe de a=3, b=1.5, c=0.4")

```


Description des paramètres

a: contrôle le taux de croissance,
b: agit comme un décalage vertical,
c: modifie la non-linéarité de la croissance


__Épidémiologie__
Propagation des Maladies : Ce modèle peut également être utilisé pour modéliser la propagation des maladies infectieuses où le taux de propagation est influencé de manière non linéaire par le nombre d'individus infectés

__Dynamique des Populations__
Croissance des Populations : Ce modèle peut être utilisé pour modéliser la croissance des populations biologiques. 



__Exercise 2.2 Behavior of the parameters of the model__

1 ) La sensibilité des paramètres fait référence à la manière dont de petites variations des paramètres d'un modèle ou d'une équation affectent le comportement ou les résultats de ce modèle. En d'autres termes, si un paramètre change légèrement, la sensibilité mesure l'ampleur de la variation dans la sortie du modèle. Un système est dit sensible à un paramètre si une petite modification de ce dernier entraîne un changement significatif dans les résultats, tandis qu'il est insensible ou robuste si ces variations ont peu d'effet. La sensibilité est importante pour comprendre la stabilité d'un modèle et identifier les paramètres critiques qui influencent fortement le système."""

2) 


Approche : Fixer b et c à des valeurs constantes, puis augmenter ou diminuer progressivement a (par exemple, tester des valeurs dans l'intervalle [2,5].
Observer comment la vitesse de croissance de la suite change.
Sensibilité : Si de petites variations de a entraînent de grandes différences dans la croissance ou la divergence de la suite, cela indique une grande sensibilité au paramètrea.


```{r}
# Générer les valeurs des paramètres à partir des intervalles
a_values <- seq(from = 2, to = 5, length.out = 5)
b_values <- seq(from = 1, to = 5, length.out = 5)
c_values <- seq(from = 0.1, to = 0.9, length.out = 5)
U0_valeus<-seq(from = 1, to = 5, length.out = 5)
# Créer une grille de combinaisons possibles des paramètres
param_grid <- expand.grid(a = a_values, b = b_values, c = c_values, U0=U0_valeus)
param_grid
```



```{r}
calculate_gradient <- function(N, a, b, c, U0) {
  h_values <- seq(0.001, 0.05, length.out = 10) # Définir une plage de h
  fU <- Generate_output(N, a, b, c, U0)
  
  # Matrices pour stocker les variations pour chaque h
  variations_a <- matrix(0, nrow = length(fU), ncol = length(h_values))
  variations_b <- matrix(0, nrow = length(fU), ncol = length(h_values))
  variations_c <- matrix(0, nrow = length(fU), ncol = length(h_values))
  
  # Calcul des variations pour chaque h
  for (i in seq_along(h_values)) {
    h <- h_values[i]
    variations_a[, i] <- (Generate_output(N, a + h, b, c, U0) - fU) / h
    variations_b[, i] <- (Generate_output(N, a, b + h, c, U0) - fU) / h
    variations_c[, i] <- (Generate_output(N, a, b, c + h, U0) - fU) / h
  }
  
  # Calcul de la moyenne des variations pour chaque paramètre
  gradient_a <- rowMeans(variations_a)
  gradient_b <- rowMeans(variations_b)
  gradient_c <- rowMeans(variations_c)
  
  # Combiner les résultats en une matrice
  gradient_f <- round(cbind(gradient_a, gradient_b, gradient_c, U0),2)
  
  return(gradient_f)
}

```



```{r}
test_calculate_gradient <- function() {
  # Paramètres pour le test
  N <- 3
  a <- 2
  b <- 2
  c <- 1
  U0 <- 1
  
  # Résultat attendu (valeurs connues ou calculées manuellement)
  waited <- c(
    0.00, 0.00, 0.00, 1,  # Ligne 1
    1.00, 1.00, 0.00, 1,  # Ligne 2
    6.03, 3.00, 11.29, 1, # Ligne 3
    22.20, 7.00, 72.05, 1 # Ligne 4
  )
  
  # Résultat obtenu
  gradients <- calculate_gradient(N, a, b, c, U0)
  fun_out <- as.vector(t(gradients)) # Transformer la matrice en vecteur ligne
  
  # Comparaison des résultats
  if (all.equal(waited, fun_out, tolerance = 1e-2)) {
    return("OK")
  } else {
    return("Error...")
  }
}

# Exécuter le test
test_calculate_gradient()

```


```{r}
# Calculer la norme du gradient
calculate_gradient_norm <- function(gradients) {
  # Calcul de la norme pour chaque ligne
  norms <- apply(gradients[,1:3], 1, function(row) sqrt(sum(row^2)))
  
  return(norms)
}
```




```{r}
test_calculate_gradient_norm <- function() {
  # Gradients d'entrée pour le test (matrice)
  gradients <- matrix(
    c(
      0.00, 0.00, 0.00, 1,  # Ligne 1
      1.00, 1.00, 0.00, 1,  # Ligne 2
      6.03, 3.00, 11.29, 1, # Ligne 3
      22.20, 7.00, 72.05, 1 # Ligne 4
    ),
    ncol = 4,
    byrow = TRUE
  )
  
  # Résultat attendu (valeurs calculées manuellement)
  waited <- c(
    0.00,                     # Norme de la ligne 1
    sqrt(1.00^2 + 1.00^2),    # Norme de la ligne 2
    sqrt(6.03^2 + 3.00^2 + 11.29^2), # Norme de la ligne 3
    sqrt(22.20^2 + 7.00^2 + 72.05^2) # Norme de la ligne 4
  )
  
  # Résultat obtenu
  fun_out <- calculate_gradient_norm(gradients)
  
  # Comparaison des résultats
  if (all.equal(waited, fun_out, tolerance = 1e-2)) {
    return("OK")
  } else {
    return("Error...")
  }
}

# Exécuter le test
test_calculate_gradient_norm()

```


```{r}
calculate_sensitivity_indicateur <- function(gradients) {
  # Vérifier si la matrice des gradients est valide
  if (is.null(gradients) || nrow(gradients) == 0 || ncol(gradients) < 3) {
    stop("La matrice des gradients doit contenir au moins 3 colonnes et une ou plusieurs lignes.")
  }
  
  # Calculer la norme des gradients pour chaque ligne
  norms <- calculate_gradient_norm(gradients)
  
  # Éviter les divisions par zéro en ajoutant une petite valeur epsilon
  epsilon <- 1e-6
  sensitivity_values <- rowSums(gradients[, 1:3]) / (norms + epsilon)
  
  # Calculer l'indicateur global en prenant la médiane des sensibilités
  median_indicator <- round(median(sensitivity_values, na.rm = TRUE), 2)
  
  return(median_indicator)
}


```



```{r}
test_calculate_sensitivity_indicateur <- function() {
  # Définir un exemple de gradients (entrée)
  test_gradients <- matrix(c(
    1, 2, 3,   # Ligne 1
    4, 5, 6,   # Ligne 2
    7, 8, 9    # Ligne 3
  ), ncol = 3, byrow = TRUE)
  
  # Résultat attendu pour cet exemple
  # Normes : sqrt(1^2 + 2^2 + 3^2), sqrt(4^2 + 5^2 + 6^2), sqrt(7^2 + 8^2 + 9^2)
  # Sensibilités relatives : rowSums(gradients) / norms
  # Indicateur : médiane des sensibilités relatives
  expected_indicator <- round(median(c(
    sum(c(1, 2, 3)) / sqrt(sum(c(1, 2, 3)^2)), # Ligne 1
    sum(c(4, 5, 6)) / sqrt(sum(c(4, 5, 6)^2)), # Ligne 2
    sum(c(7, 8, 9)) / sqrt(sum(c(7, 8, 9)^2))  # Ligne 3
  )), 2)
  
  # Appeler la fonction avec l'entrée
  result_indicator <- calculate_sensitivity_indicateur(test_gradients)
  
  # Vérifier le résultat
  if (all.equal(result_indicator, expected_indicator, tolerance = 1e-2)) {
    return("OK")
  } else {
    return(paste("Test Échec: Attendu =", expected_indicator, "Obtenu =", result_indicator))
  }
}
test_calculate_sensitivity_indicateur()
```


Stock des Indicateur
```{r}
Stock_sensitivity_params <- function(param_list, N) {
  # Initialiser une liste vide pour stocker les résultats
  stock <- data.frame(a = numeric(), b = numeric(), c = numeric(), U0 = numeric(), Indicator = numeric())
  
  # Parcourir chaque ensemble de paramètres dans la liste
  for (params in param_list) {
    a <- params$a
    b <- params$b
    c <- params$c
    U0 <- params$U0
    
    # Calculer les gradients pour cet ensemble de paramètres
    gradients <- calculate_gradient(N, a, b, c, U0)
    
    # Calculer l'indicateur de sensibilité pour les gradients obtenus
    indicator <- round(calculate_sensitivity_indicateur(gradients),2)
    
    # Ajouter les résultats à la base stock
    stock <- rbind(stock, data.frame(a = a, b = b, c = c, U0 = U0, Indicator = indicator))
  }
  
  return(stock)
}

```




```{r}
get_sensibility <- function(stock) {
  # Calculer la médiane de la colonne Indicator
  median_value <- median(stock$Indicator, na.rm = TRUE)
  
  # Trouver l'indice de la ligne dont la valeur d'Indicator est la plus proche de la médiane
  closest_index <- which.min(abs(stock$Indicator - median_value))
  
  # Retourner la ligne correspondante
  selected_row <- stock[closest_index, ]
  
  return(selected_row)
}

```


```{r}
test_get_sensibility <- function() {
  # Créer un dataframe de test avec des valeurs prédéfinies
  stock <- data.frame(
    a = c(1, 2, 3, 4, 5),
    b = c(10, 15, 20, 25, 30),
    c = c(0.5, 1, 1.5, 2, 2.5),
    U0 = c(1, 2, 3, 4, 5),
    Indicator = c(2, 7, 12, 3, 9)  # Valeurs arbitraires
  )
  
  # Résultat attendu : médiane = 7, la ligne la plus proche de 7 est la 2ème (Indice 2)
  expected_result <- stock[2, ]
  
  # Appeler la fonction get_sensibility
  result <- get_sensibility(stock)
  
  # Vérification si le résultat correspond à ce qui est attendu
  if (all(result == expected_result)) {
    return("ok")
  } else {
    return("Erreur : Résultat incorrect")
  }
}

# Exécution du test
test_get_sensibility()

```


```{r}
# Exemple d'utilisation : définir plusieurs ensembles de paramètres
param_list <- list(
  list(a = 2, b = 2.5, c = 0.4, U0 = 1),
  list(a = 3.0, b = 4.0, c = 0.6, U0 = 1),
  list(a = 2.0, b = 3.0, c = 0.5, U0 = 1),
  list(a = 5.0, b = 5.0, c = 0.7, U0 = 1)
)

# Trouver les paramètres avec la meilleure sensibilité
Param_sim <- get_sensibility(Stock_sensitivity_params(param_list, N = 100))
print(Param_sim)

```

__Exercise 2.3 Distribution of the parameters__


Les paramètres d'entrée du modèle sont :

_a_ : Ce paramètre contrôle le facteur de croissance multiplicatif de la suite Un. Il influence directement la manière dont Un. Plus la valeur de a est élevée, plus la suite croît rapidement.

_b_ : Ce paramètre représente un terme constant qui est ajouté à chaque itération. Il détermine le biais ou la contribution fixe à chaque étape de la suite. Un b élevé entraîne.

_c_ : Ce paramètre contrôle la puissance à laquelle Un est élevé. Il modifie la manière non linéaire dont un influence sa propre évolution. Un c plus grand accentue les écarts, tandis qu'un c plus petit atténue la croissance de Un

_U0_  : La condition initiale, qui définit la valeur de départ de la suite U. C'est la première valeur qui déclenche l'évolution de la suite selon les paramètresa,b et c

_N_ : Le nombre d'itérations ou d'étapes, représentant la durée sur laquelle la suite est calculée.

_Sorties_ :
Les sorties du modèle sont : _Un_: La suite de valeurs obtenue après N itérations, où chaque valeur  Un dépend des paramètres a,b et c
ainsi que de la valeur précédente Un−1. C'est la sortie principale du modèle qui décrit l'évolution de la suite en fonction des entrées.



__Lois des paramètres a,b et c.__
Je simule une loi normale  pour mes paramètre
Mon param


```{r}
set.seed(123) # Pour des résultats reproductibles
n <- 50 # Nombre de simulations souhaité

# Simulation à partir des lois normales
sim_a <- rnorm(n, mean = 3, sd = 0.2)
sim_b <- rnorm(n, mean = 4, sd = 0.3)
sim_c <- rnorm(n, mean = 0.6, sd = 0.05)

sim_U0 <- round(runif(5, min = 1, max = 10),0)

# Créer les histogrammes
par(mfrow = c(1, 3))  # Pour afficher les 3 histogrammes côte à côte

hist(sim_a, main = "Histogramme de a", xlab = "a", col = "lightblue", breaks = 30)
hist(sim_b, main = "Histogramme de b", xlab = "b", col = "lightgreen", breaks = 30)
hist(sim_c, main = "Histogramme de c", xlab = "c", col = "lightcoral", breaks = 30)

```



__Exercise 3.1 The notion of noisy data__

__1. Notion de bruit dans la collecte de données__
Le bruit en collecte de données désigne les variations aléatoires ou les informations non pertinentes qui perturbent les données recueillies. Ces perturbations ne reflètent pas le phénomène réel mesuré. Le bruit peut provenir de sources variées, telles que des erreurs de mesure, des variations environnementales ou des biais dans les instruments de collecte.

__2. Problèmes causés par le bruit__
La présence de bruit peut entraîner plusieurs problèmes :

__Baisse de la précision__ : Le bruit rend les données moins fiables et rend difficile la distinction entre les variations aléatoires et les tendances réelles.
Difficulté à interpréter : Le bruit complique l'analyse, car il peut masquer les relations importantes entre les variables.
Modèles biaisés : En présence de bruit important, les modèles statistiques peuvent produire des résultats inexacts ou non représentatifs.

__3. Loi de probabilité du composant aléatoire__
Le composant aléatoire, c'est-à-dire le bruit, peut souvent être modélisé par une loi normale (gaussienne), car les erreurs et perturbations aléatoires ont tendance à suivre une distribution symétrique autour de zéro avec une variance spécifique. Cependant, d'autres distributions peuvent être utilisées selon la nature du bruit (loi uniforme pour des erreurs constantes, loi exponentielle pour des erreurs asymétriques, etc.). La loi normale est souvent privilégiée car elle reflète bien les erreurs aléatoires dans de nombreux contextes.

__Exercise 3.2 Generation of a learning database__

```{r}
# Simulation des paramètres pour n individus
set.seed(123)  # Pour la reproductibilité
M <- 100  # Nombre d'individus
N <- 40  # Nombre d'itérations

# Simulation des paramètres a, b, c, et U0
sim_a <- round(rnorm(M, mean = 3, sd = 0.2), 2)  # Simulation des paramètres a
sim_b <- round(rnorm(M, mean = 4, sd = 0.3), 2)  # Simulation des paramètres b
sim_c <- round(rnorm(M, mean = 0.6, sd = 0.05), 2)  # Simulation des paramètres c
sim_Uo <- round(runif(M, min = 1, max = 10), 2)  # Simulation des valeurs initiales de U0

# Générer une base de données avec les valeurs de a, b, c et U[0], U[1], ..., U[N]
base_root <- data.frame(matrix(ncol = N + 4, nrow = M))  # Base vide (n lignes, N+4 colonnes)

# Noms des colonnes : a, b, c, U[0], ..., U[100]
colnames(base_root) <- c("a", "b", "c", paste0("U", 0:N))

# Remplir la base avec les valeurs simulées de a, b, c et les valeurs de U
for (i in 1:M) {
  base_root[i, 1] <- sim_a[i]  # Valeur de a
  base_root[i, 2] <- sim_b[i]  # Valeur de b
  base_root[i, 3] <- sim_c[i]  # Valeur de c
  base_root[i, 4] <- sim_Uo[i]  # Valeur de Uo
  
  # Calculer les valeurs de U à l'aide de la fonction Generate_output (assurez-vous qu'elle est définie)
  U_values <- round(Generate_output(N, sim_a[i], sim_b[i], sim_c[i], sim_Uo[i]), 2)
  
  # Ajouter les valeurs de U dans la base
  base_root[i, 4:(N + 4)] <- U_values
}

# Restructurer les données pour le graphique
base_root %>%
  mutate(Individu = 1:M) %>%               # Ajouter la colonne 'Individu'
  melt(id.vars = c("a", "b", "c", "Individu"), 
       variable.name = "Iteration", 
       value.name = "U") %>%                # Restructurer le dataframe
  mutate(Iteration = as.numeric(gsub("U", "", Iteration))) %>%  # Convertir la colonne 'Iteration' en numérique
  ggplot(aes(x = Iteration, y = U, color = factor(Individu))) + # Graphique ggplot
  geom_line(linewidth = 1) +                     # Remplacer 'size' par 'linewidth'
  labs(title = "Évolution de U[n] pour chaque individu",
       x = "Itération (N)", 
       y = "Valeur de U[N]",
       color = "Individu") +
  theme_minimal()

```



```{r}
set.seed(123)  # Pour la reproductibilité
# Générer une base de données avec les valeurs de a, b, c et U[0], U[1], ..., U[100]
base_brutee <- data.frame(matrix(ncol = N + 4, nrow = M))  # Base vide (n lignes, N+4 colonnes)

# Noms des colonnes : a, b, c, U[0], ..., U[100]
colnames(base_brutee) <- c("a", "b", "c", paste0("U", 0:N))

# Remplir la base avec les valeurs simulées de a, b, c et les valeurs de U
for (i in 1:M) {
  base_brutee[i, 1] <- sim_a[i]  # Valeur de a
  base_brutee[i, 2] <- sim_b[i]  # Valeur de b
  base_brutee[i, 3] <- sim_c[i]  # Valeur de c
  base_brutee[i, 4] <- sim_Uo[i]  # Valeur de Uo
  U_values_bruité <- Generate_output(N, sim_a[i], sim_b[i], sim_c[i], sim_Uo[i]) + rnorm(N+1, mean = 0, sd = 1) # Générer   
  base_brutee[i, 4:(N + 4)] <- round(U_values_bruité,2)  # Remplir les colonnes U[0] à U[100]
}

base_brutee %>%
  mutate(Individu = 1:M) %>%               # Ajouter la colonne 'Individu'
  melt(id.vars = c("a", "b", "c", "Individu"), 
       variable.name = "Iteration", 
       value.name = "U") %>%                # Restructurer le dataframe
  mutate(Iteration = as.numeric(gsub("U", "", Iteration))) %>%  # Convertir la colonne 'Iteration' en numérique
  ggplot(aes(x = Iteration, y = U, color = factor(Individu))) + # Graphique ggplot
  geom_line(size = 1) +                     # Tracer les courbes pour chaque individu
  labs(title = "Évolution de U[n] bruité pour chaque individu",
       x = "Itération (N)", 
       y = "Valeur de U[N]",
       color = "Individu") +
  theme_minimal()    
```

```{r}
set.seed(123)  # Pour la reproductibilité
# Nombre de simulations
M <- 100 # Ajustez ce nombre selon vos besoins
N <- 40   # Nombre d'itérations pour Generate_output

# Simulation des paramètres
sim_a <- round(rnorm(M, mean = 3, sd = 0.2), 2)  # Simulation des paramètres a
sim_b <- round(rnorm(M, mean = 4, sd = 0.3), 2)  # Simulation des paramètres b
sim_c <- round(rnorm(M, mean = 0.6, sd = 0.05), 2)  # Simulation des paramètres c
sim_U0 <- round(runif(M, min = 1, max = 10), 2)  # Simulation des valeurs initiales de U0
```


```{r}
set.seed(123)  # Pour la reproductibilité

# Génération des résultats et création de la DataFrame
Output_Curves <- data.frame(
  a = sim_a,
  b = sim_b,
  c = sim_c,
  U0 = sim_U0
)

# Calcul des sorties Un et ajout comme une colonne
Output_Curves$Un <- lapply(1:nrow(Output_Curves), function(i) {
  Generate_output(N, Output_Curves$a[i], Output_Curves$b[i], Output_Curves$c[i], Output_Curves$U0[i])
})


# Affichage de la DataFrame
head(Output_Curves)

```



```{r}
# Génération des résultats et création de la DataFrame
Main_Learning_Base <- data.frame(
  a = sim_a,
  b = sim_b,
  c = sim_c,
  U0 = sim_U0
)

# Calcul des sorties Un et ajout comme une colonne
Main_Learning_Base$Un <- lapply(1:nrow(Main_Learning_Base), function(i) {
  # Générer les valeurs de Un avec Generate_output
  generated_values <- Generate_output(N, Main_Learning_Base$a[i], Main_Learning_Base$b[i], Main_Learning_Base$c[i], Main_Learning_Base$U0[i])
  
  generated_values[-1] <- generated_values[-1] + rnorm(N, mean = 0, sd = 1)
  
  return(generated_values)
})

```


# Build SQL Base


```{r}
# Établir la connexion
con <- dbConnect(RMySQL::MySQL(), 
                 dbname = "Mlearn",  # Nom de la base de données
                 host = "127.0.0.1",    # Adresse locale du serveur MySQL
                 port = 3306,           # Port par défaut pour MySQL
                 user = "root",         # Nom d'utilisateur
                 password = "less@intsARS19") # Mot de passe réel

# Vérification de la connexion
if (!is.null(con)) {
  cat("Connexion réussie à la base de données 'mlearning' !\n")
} else {
  cat("Erreur lors de la connexion.\n")
}

```



# table des paramètres

```{r}
param<-Main_Learning_Base[, 1:4]
```


```{r}
# Créer la table Parameters
dbExecute(con, "
  CREATE TABLE IF NOT EXISTS Parameters (
    id INT AUTO_INCREMENT PRIMARY KEY,
    a FLOAT NOT NULL,
    b FLOAT NOT NULL,
    c FLOAT NOT NULL,
    U0 FLOAT NOT NULL
    
  )
")

```
__Créer la table NormalData__

```{r}
# Créer la table NormalData avec l'ID comme clé primaire et la clé étrangère pour parameter_id
dbExecute(con, "
  CREATE TABLE IF NOT EXISTS NormalData (
    id INT AUTO_INCREMENT PRIMARY KEY,  -- Définir id comme clé primaire
    parameter_id INT NOT NULL,          -- Clé étrangère
    Un JSON NOT NULL,                   -- Colonne JSON pour stocker Un
    FOREIGN KEY (parameter_id) REFERENCES Parameters(id) ON DELETE CASCADE  -- Référence à Parameters
  )
")


```

__Créer la table NoiseData__

```{r}
# Créer la table NoiseData
dbExecute(con, "
  CREATE TABLE IF NOT EXISTS NoiseData (
    id INT AUTO_INCREMENT PRIMARY KEY,
    parameter_id INT NOT NULL,
    Un JSON NOT NULL,
    FOREIGN KEY (parameter_id) REFERENCES Parameters(id) ON DELETE CASCADE
  )
")


```


```{r}
# Désactiver les contraintes de clé étrangère
dbExecute(con, "SET foreign_key_checks = 0")

# Vider la table Parameters
dbExecute(con, "TRUNCATE TABLE Parameters")

# Réactiver les contraintes de clé étrangère
dbExecute(con, "SET foreign_key_checks = 1")


```
__Charger les données de la table Parameters__
```{r}
# Préparer les données à insérer, en excluant l'`id`
for (i in 1:nrow(param)) {
  query <- sprintf("INSERT INTO Parameters (a, b, c, U0) VALUES (%f, %f, %f, %f)",
                   param$a[i], param$b[i], param$c[i], param$U0[i])
  dbExecute(con, query)
}

```

__Charger les données de la table Output_Curves __
```{r}
# Créer la colonne 'parameter_id' qui contient des valeurs de 1 à N
Output_Curves$parameter_id <- 1:nrow(Output_Curves)

# Préparer les données à insérer dans la table NormalData
for (i in 1:nrow(Output_Curves)) {
  # Convertir la colonne Un en chaîne JSON
  Un_json <- toJSON(Output_Curves$Un[[i]], auto_unbox = TRUE)  # auto_unbox pour convertir en un tableau JSON correct
  
  # Construire la requête SQL
  query <- sprintf("INSERT INTO NormalData (parameter_id, Un) VALUES (%d, '%s')",
                   Output_Curves$parameter_id[i], Un_json)  # '%s' pour insérer la chaîne JSON

  # Exécuter la requête d'insertion
  dbExecute(con, query)
}

```



__Charger les données de la table Main_Learning_Base __
```{r}
# Créer la colonne 'parameter_id' qui contient des valeurs de 1 à N
Main_Learning_Base$parameter_id <- 1:nrow(Main_Learning_Base)

# Préparer les données à insérer dans la table NormalData
for (i in 1:nrow(Main_Learning_Base)) {
  # Convertir la colonne Un en chaîne JSON
  Un_j <- toJSON(Main_Learning_Base$Un[[i]], auto_unbox = TRUE)  # auto_unbox pour convertir en un tableau JSON correct
  
  # Construire la requête SQL
  query <- sprintf("INSERT INTO NoiseData (parameter_id, Un) VALUES (%d, '%s')",
                   Main_Learning_Base$parameter_id[i], Un_j)  # '%s' pour insérer la chaîne JSON

  # Exécuter la requête d'insertion
  dbExecute(con, query)
}
```







# Definir les base d'entrainement et de test

```{r}
# Supposons que Main_Learning_Base est déjà chargé et contient M lignes
set.seed(123)  # Pour la reproductibilité

# Déterminer le nombre de lignes pour chaque ensemble
train_size <- floor(0.7 * M)  # 70% pour l'entraînement
test_size <- M - train_size     # 30% pour le test

# Échantillonner les indices pour l'ensemble d'entraînement
train_indices <- sample(1:M, size = train_size, replace = FALSE)

# Créer les ensembles d'entraînement et de test
Main_Training_Base <- Main_Learning_Base[train_indices, ]  # Ensemble d'entraînement
Main_Test_Base <- Main_Learning_Base[-train_indices, ]      # Ensemble de test
```


Intérêt de cette approche :
L'approche vise à ajuster les paramètres d'un modèle (a,b,c) pour qu'il s'aligne au mieux avec des données réelles bruitées. Cela permet de trouver les paramètres qui minimisent la différence entre les prédictions du modèle et les observations. Ce processus est essentiel en modélisation pour :

Améliorer la précision des prédictions du modèle ;
Comprendre les relations sous-jacentes dans les données ;
Réduire l'erreur et ajuster le modèle aux particularités des données bruitées.



je veux ajouter d'autre ligne pour repondre à la fonction avec All_Uo

```{r}

Uo <- c(8.21, 1.572, 3.197)
a1<-1.72
b1<-2.72
c1<-0.80

# Vérifier et mettre à jour la base
for (u0 in Uo) {
  # Vérifier si les paramètres (a, b, c, U0) existent déjà dans `param`
  existing_row <- param[param$a == a1 & param$b == b1 & param$c == c1 & param$U0 == u0, ]
  
  if (nrow(existing_row) == 0) {  # Si les paramètres n'existent pas déjà
    # Ajouter les paramètres dans `param`
    new_param <- data.frame(a = a1, b = b1, c = c1, U0 = u0)
    param <- rbind(param, new_param)
    Nparam <-nrow(param)
    
    # Générer la suite avec bruit
    U_val_bruité <- Generate_output(N, a1, b1, c1, u0) 
    U_val_bruité[-1]<-U_val_bruité[-1]+ rnorm(N,mean=0,sd=1)
    # Ajouter les données dans Main_Learning_Base
    new_row <- data.frame(a = a1, b = b1, c = c1, U0 = u0, Un = I(list(U_val_bruité)), parameter_id = Nparam)
    Main_Learning_Base <- rbind(Main_Learning_Base, new_row)
    
# Générer la suite sans bruit
    U_val <- Generate_output(N, a1, b1, c1, u0)
    
    # Ajouter les données dans Output_Curves
    new_curve <- data.frame(a = a1, b = b1, c = c1, U0 = u0, Un = I(list(U_val)), parameter_id = Nparam)
    Output_Curves <- rbind(Output_Curves, new_curve)
  }
}

```


### 7. Algorithme DIRECT

L'algorithme **DIRECT** (DIviding RECTangles) est une méthode d'optimisation globale utilisée pour trouver les valeurs optimales de paramètres dans un espace multidimensionnel sans nécessiter de dérivées. Il fonctionne en divisant systématiquement l'espace de recherche en sous-espaces (rectangles) et en évaluant leur potentiel à contenir une solution optimale. L'algorithme privilégie l'exploration des régions prometteuses et abandonne celles qui sont moins susceptibles de donner de bons résultats.

### 8. Utilisation de DIRECT pour ajuster les paramètres \(a\), \(b\) et \(c\)

L'algorithme DIRECT est utilisé pour ajuster les paramètres \(a\), \(b\) et \(c\) afin de minimiser l'écart entre les valeurs observées et prédites par un modèle. L'approche consiste à explorer l'espace des paramètres en cherchant les valeurs qui minimisent l'erreur, en utilisant la fonction `f_obj` pour calculer cet écart. Le processus d'optimisation vise à ajuster ces paramètres pour améliorer la précision du modèle.




```{r}
RelDiff <- function(y_observe, y_pred) {
  return(rowMeans(abs(y_observe - y_pred)))
}
```


```{r}
test_RelDiff <- function() {
  # Définir des valeurs d'observation et prédites
  y_observe <- matrix(c(1, 2, 3, 4), nrow = 2, byrow = TRUE)  
  y_pred <- matrix(c(1, 2.5, 3, 4.5), nrow = 2, byrow = TRUE)
  
  # Résultat attendu : moyenne des différences absolues
  waited <- c(0.25, 0.25)  

  # Appel de la fonction RelDiff
  fun_out <- RelDiff(y_observe, y_pred)

  # Comparer les résultats attendus avec ceux obtenus
  if(all(waited==fun_out)){
   return("OK")
 }else{
   return("Error...")
   }
}

# Appel de la fonction de test
test_RelDiff()

```



```{r}
f_obj <- function(A, B, C) {
  # Filtrer les lignes de la base principale
  filtered_row <- subset(Main_Training_Base, a == A & b == B & c == C)
  
  if (nrow(filtered_row) == 0) {
    return(paste0('les paramètres renseignés ne sont pas dans la base')) # Retourner une valeur élevée si aucun filtre ne correspond
  }
  y_obser <- do.call(rbind, lapply(filtered_row$Un, function(x) unlist(x)))
  # Déterminer N comme la longueur totale de `y_obser`
  N<- ncol(y_obser)-1
  
  # Générer les prédictions
  y_pred <- t(mapply(Generate_output, N, A, B, C, filtered_row[, 4]))
  
  # Calculer l'écart
  ecart <- RelDiff(y_obser, y_pred)
  
  return(ecart)
}

```


```{r}
test_f_obj <- function() {
  # Définir les valeurs des paramètres pour le test
  A <- 3.09
  B <- 4.43
  C <- 0.70
  
  # Résultat attendu pour ces paramètres
  waited <- 0.8520035
  
  # Appeler la fonction f_obj avec les paramètres donnés
  fun_out <- f_obj(A, B, C)
  
  # Comparer le résultat de la fonction avec le résultat attendu
  if (all.equal(fun_out, waited, tolerance = 1e-6)) {
    return("OK")
  } else {
    return("Error...")
  }
}

# Appeler la fonction de test
test_f_obj()

```


```{r}
f_obj(3.09,
4.43,
2)

```




```{r}
#train parameters
params<-Main_Training_Base[, 1:3]
```

```{r}
# Define the objective function to minimize
objectif <- function(p) {
  A <- p[1]
  B <- p[2]
  C <- p[3]
  n<-nrow(p)
  
  fun <- 0
  for (i in n ) {
    fun <- fun + f_obj(A[i,], B[i,], C[i,])
  }
  fun
  
}

# Définir les bornes pour les paramètres a, b, et c
bornes_inf <- c(min(params$a), min(params$b), min(params$c))  # Bornes inférieures
bornes_sup <- c(max(params$a), max(params$b), max(params$c))  # Bornes supérieures

# Apply the DIRECT algorithm to optimize the parameters
resultat <- direct(fn = objectif, lower = bornes_inf, upper = bornes_sup, control = list(print.level = 1))

# Print the results (optimized parameters a, b, c)
print(resultat$par)  # Optimized values of a, b, and c

```

## Exercise 3.4 Stability of the parameters

```{r}
#set.seed(123)  # Pour garantir la reproductibilité du tirage
Sampl_train <- Main_Training_Base[sample(nrow(Main_Training_Base), 50, replace = TRUE), ]
```



```{r}
param_spl<-Sampl_train[,1:3]
# Définir les bornes pour les paramètres a, b, et c
bornes_inf_spl <- c(min(param_spl$a), min(param_spl$b), min(param_spl$c))  # Bornes inférieures
bornes_sup_spl <- c(max(param_spl$a), max(param_spl$b), max(param_spl$c))  # Bornes supérieures

# Apply the DIRECT algorithm to optimize the parameters
resultat_spl <- direct(fn = objectif, lower = bornes_inf_spl, upper = bornes_sup_spl, control = list(print.level = 1))

# Print the results (optimized parameters a, b, c)
print(resultat_spl$par)  # Optimized values of a, b, and c

```


```{r}
# Créer un tableau vide pour stocker les résultats des 10 itérations
results_table <- data.frame(a = numeric(10), b = numeric(10), c = numeric(10))

# Répéter l'opération 10 fois
for (i in 1:10) {
  # Tirage aléatoire avec remise de 50 courbes
  indices <- sample(nrow(Main_Training_Base), 50, replace = TRUE)
  Newbase_spl <- Main_Training_Base[indices, ]
  param_sp<-Newbase_spl[,1:3]
# Définir les bornes pour les paramètres a, b, et c
  bornes_inf_sp <- c(min(param_sp$a), min(param_sp$b), min(param_sp$c))  # Bornes inférieures
  bornes_sup_sp <- c(max(param_sp$a), max(param_sp$b), max(param_sp$c))  # Bornes   supérieures
  # Appliquer l'algorithme DIRECT pour ajuster les paramètres
  resultat_sp <- direct(fn = objectif, lower = bornes_inf_sp, upper = bornes_sup_sp, control = list(print.level = 0))
  
  # Stocker les résultats des paramètres optimisés dans le tableau
  results_table[i, ] <- round(resultat_sp$par,2)
}

# Afficher le tableau contenant les valeurs des 10 triplets
print(results_table)

```


```{r}
# Calcul de la moyenne, de la variance et de l'écart-type relatifs
moyenne <- colMeans(results_table)
variance <- apply(results_table, 2, var)
ecart_type <- sqrt(variance)

# Calcul de la variance relative et de l'écart-type relatif
# Variance relative = variance / (moyenne^2)
variance_relative <- variance / (moyenne^2)

# Ecart-type relatif = écart-type / moyenne
ecart_type_relatif <- ecart_type / moyenne

# Afficher les résultats
resultats_statistiques <- data.frame(
  Moyenne = round(moyenne,2),
  Variance_Relative = variance_relative,
  Ecart_Type_Relatif = round(ecart_type_relatif,2)
)

print(resultats_statistiques)

```

__Résultats :__
Les calculs de la **moyenne**, de la **variance relative** et de l'**écart-type relatif** pour les paramètres \(a\), \(b\) et \(c\) ont montré l'instabilité ou la stabilité de chaque paramètre selon les échantillons tirés. Des moyennes proches de la valeur optimale indiquent une bonne estimation, tandis que des valeurs faibles de variance relative et d'écart-type relatif indiquent que les paramètres sont stables et peu sensibles aux variations des données d'entraînement.

__Intérêt de l'approche :__
Cette approche permet d'évaluer la **robustesse du modèle** en tirant des échantillons avec remise. Elle offre une estimation fiable des paramètres optimisés en tenant compte de la variabilité des données. Les statistiques calculées permettent de mesurer la stabilité des paramètres, et donc la fiabilité du modèle, tout en réduisant les risques de **sur-apprentissage** lié à des fluctuations des données d'entraînement.


## Exercise 3.5 Accuracy of the adjuted model

### 1. Choix des indicateurs pour quantifier la qualité de l'ajustement du modèle

J'ai choisi d'utiliser l'**Erreur Absolue Moyenne (MAE)** comme indicateur de la qualité de l'ajustement du modèle. Le MAE est particulièrement adapté car il mesure l'erreur moyenne entre les valeurs réelles et les valeurs prédites. Il est robuste aux valeurs aberrantes et est exprimé dans les mêmes unités que la variable dépendante, ce qui le rend facile à interpréter. Contrairement aux autres indicateurs comme le MSE ou le RMSE, le MAE ne pénalise pas de manière disproportionnée les grandes erreurs, ce qui le rend plus approprié pour une analyse d'ajustement général.

### 2. Calcul de la précision du modèle sur le jeu de données d'entraînement

Le **MAE** sur le jeu de données d'entraînement se calcule à l'aide de la formule suivante :

\[
MAE_{\text{train}} = \frac{1}{n} \sum_{i=1}^{n} |y_{\text{réel}, i} - y_{\text{prédit}, i}|
\]

Où :
- \( y_{\text{réel}, i} \) représente la valeur réelle de l'observation \( i \),
- \( y_{\text{prédit}, i} \) est la valeur prédite pour l'observation \( i \),
- \( n \) est le nombre total d'observations dans le jeu de données d'entraînement.

Le **MAE** permet de quantifier l'ajustement du modèle aux données d'entraînement en indiquant la moyenne des erreurs absolues entre les valeurs réelles et prédites. Un **MAE** faible indique une bonne précision du modèle, tandis qu'un **MAE** élevé suggère que les prédictions du modèle sont moins précises.



# Train Score
```{r}
a_dj<-3
b_dj<-4.18
c_dj<-0.62
out_size<-40
# Fonction pour calculer le MAE pour chaque ligne
calculate_mae_per_row <- function(y_real_data, y_pred_data) {
  # Vérifier que les dimensions des matrices correspondent
  if (nrow(y_real_data) != nrow(y_pred_data)) {
    stop("Les matrices y_real_data et y_pred_data doivent avoir le même nombre de lignes.")
  }
  
  # Calcul du MAE pour chaque ligne
  mae_per_row <- apply(cbind(y_real_data, y_pred_data), 1, function(row) {
    # Extraire la ligne de valeurs réelles et prédites
    y_real <- row[1:ncol(y_real_data)]
    y_pred <- row[(ncol(y_real_data) + 1):length(row)]
    
    # Calcul de l'erreur absolue pour la ligne
    mae <- mean(abs(y_real - y_pred))
    return(mae)
  })
  
  # Calcul de la moyenne des MAE
  mae_global <- mean(mae_per_row)
  
  return(mae_global)
}

# Exemple d'utilisation
# Matrices de valeurs réelles et prédites (par exemple 3 observations avec 2 variables)
y_real_data <- do.call(rbind, lapply(Main_Training_Base$Un, function(x) unlist(x)))
y_pred_data <- t(mapply( Generate_output ,out_size,a_dj, b_dj ,c_dj, Main_Training_Base$U0))

# Calcul du MAE global
mae_train <- calculate_mae_per_row(y_real_data, y_pred_data)

# Affichage du résultat
print(paste("Le MAE train est :", mae_train))

```



# test Score
```{r}
a_dj<-3
b_dj<-4.18
c_dj<-0.62
out_size<-40

# Exemple d'utilisation
# Matrices de valeurs réelles et prédites (par exemple 3 observations avec 2 variables)
y_real_test <- do.call(rbind, lapply(Main_Test_Base$Un, function(x) unlist(x)))
y_pred_test <- t(mapply( Generate_output ,out_size,a_dj, b_dj ,c_dj, Main_Test_Base$U0))

# Calcul du MAE global
mae_test <- calculate_mae_per_row(y_real_test, y_pred_test)

# Affichage du résultat
print(paste("Le MAE test est :", mae_test))

```


### Description des résultats

Les valeurs obtenues pour le **MAE** (Erreur Absolue Moyenne) sur les jeux de données d'entraînement et de test sont les suivantes :
- **MAE train** : 6.22
- **MAE test** : 6.45

Ces résultats montrent que l'erreur moyenne absolue est légèrement plus élevée sur le jeu de test (6.45) que sur le jeu d'entraînement (6.22). Cela peut indiquer que le modèle est légèrement mieux ajusté aux données d'entraînement qu'aux données de test. L'écart entre ces deux valeurs suggère que le modèle pourrait être légèrement surajusté (overfitting) aux données d'entraînement, bien que l'écart ne soit pas très important. En général, un **MAE** relativement faible pour les deux jeux de données indique que le modèle fournit des prédictions assez précises.


## Exercise 3.6 Comparison of different algorithms


`







```{r}
adam <- function(y_observe_list, U0_list, N, learning_rate, max_iter, beta1 = 0.9, beta2 = 0.999, epsilon = 1e-8) {
  # Initialize parameters
  a <- 0
  b <- 0
  c <- 0
  
  # Initialize moment estimates
  m_a <- m_b <- m_c <- 0
  v_a <- v_b <- v_c <- 0
  
  # Initialize bias correction
  beta1_t <- beta2_t <- 1
  
  # Store cost history
  cost_history <- numeric(max_iter)
  
  for (iter in 1:max_iter) {
    # Generate predictions
    y_pred_list <- lapply(U0_list, function(U0) Generate_output(N, a, b, c, U0))
    
    # Compute the cost
    cost <- mean(sapply(seq_along(U0_list), function(i) {
      y_pred <- y_pred_list[[i]]
      y_observe <- y_observe_list[i, ]
      sum((y_pred - y_observe)^2) / length(y_observe)
    }))
    cost_history[iter] <- cost
    
    # Compute gradients
    grad_a <- mean(sapply(seq_along(U0_list), function(i) {
      U0 <- U0_list[i]
      y_pred <- y_pred_list[[i]]
      y_observe <- y_observe_list[i, ]
      sum(2 * (y_pred - y_observe) * U0^c) / length(y_observe)
    }))
    
    grad_b <- mean(sapply(seq_along(U0_list), function(i) {
      y_pred <- y_pred_list[[i]]
      y_observe <- y_observe_list[i, ]
      sum(2 * (y_pred - y_observe)) / length(y_observe)
    }))
    
    grad_c <- mean(sapply(seq_along(U0_list), function(i) {
      U0 <- U0_list[i]
      y_pred <- y_pred_list[[i]]
      y_observe <- y_observe_list[i, ]
      sum(2 * (y_pred - y_observe) * log(U0) * a * U0^c) / length(y_observe)
    }))
    
    # Update moments
    m_a <- beta1 * m_a + (1 - beta1) * grad_a
    m_b <- beta1 * m_b + (1 - beta1) * grad_b
    m_c <- beta1 * m_c + (1 - beta1) * grad_c
    
    v_a <- beta2 * v_a + (1 - beta2) * grad_a^2
    v_b <- beta2 * v_b + (1 - beta2) * grad_b^2
    v_c <- beta2 * v_c + (1 - beta2) * grad_c^2
    
    # Bias correction
    beta1_t <- beta1_t * beta1
    beta2_t <- beta2_t * beta2
    
    m_a_hat <- m_a / (1 - beta1_t)
    m_b_hat <- m_b / (1 - beta1_t)
    m_c_hat <- m_c / (1 - beta1_t)
    
    v_a_hat <- v_a / (1 - beta2_t)
    v_b_hat <- v_b / (1 - beta2_t)
    v_c_hat <- v_c / (1 - beta2_t)
    
    # Update parameters
    a <- a - learning_rate * m_a_hat / (sqrt(v_a_hat) + epsilon)
    b <- b - learning_rate * m_b_hat / (sqrt(v_b_hat) + epsilon)
    c <- c - learning_rate * m_c_hat / (sqrt(v_c_hat) + epsilon)
    
    # Print cost every 100 iterations
    if (iter %% 100 == 0) {
      cat("Iteration:", iter, " Cost:", cost, "\n")
    }
  }
  
  return(list(a = a, b = b, c = c, cost_history = cost_history))
}

# Example usage
N <- 40
U0_list <- Main_Training_Base$U0  # List of initial values
y_observe_list <- do.call(rbind, lapply(Main_Training_Base$Un, function(x) unlist(x)))  # Observations matrix
learning_rate <- 0.001
max_iter <- 1000

result <- adam(y_observe_list, U0_list, N, learning_rate, max_iter)
cat("Final Parameters: a =", result$a, "b =", result$b, "c =", result$c, "\n")

```



```{r}
sgd <- function(y_observe_list, U0_list, N, learning_rate, max_iter) {
  # Initialisation des paramètres
  a <- 0  # Initialisation dans l'intervalle [2, 5]
  b <- 0  # Initialisation dans l'intervalle [1, 5]
  c <- 0  # Initialisation dans l'intervalle [0.1, 0.9]
  
  cost_history <- numeric(max_iter)  # Stockage des coûts
  
  for (iter in 1:max_iter) {
    # Sélection d'un exemple aléatoire
    i <- sample(1:length(U0_list), 1)
    U0 <- U0_list[[i]]
    y_observe <- y_observe_list[i, ]
    
    # Calculer les prédictions
    y_pred <- Generate_output(N, a, b, c, U0)
    
    # Calcul des gradients
    grad_a <- sum(2 * (y_pred - y_observe) * U0^c) / length(y_observe)
    grad_b <- sum(2 * (y_pred - y_observe)) / length(y_observe)
    grad_c <- sum(2 * (y_pred - y_observe) * log(U0) * a * U0^c) / length(y_observe)
    
    # Mise à jour des paramètres avec contraintes
    a <- max(min(a - learning_rate * grad_a, max(params$a)), min(params$a))
    b <- max(min(b - learning_rate * grad_b, max(params$b)), min(params$b))
    c <- max(min(c - learning_rate * grad_c, max(params$c)), min(params$c))
  
    # Calcul du coût
    cost <- sum((y_pred - y_observe)^2) / length(y_observe)
    cost_history[iter] <- cost
    
    # Afficher le coût toutes les 100 itérations
    if (iter %% 100 == 0) {
      cat("Iteration:", iter, " Cost:", cost, 
          " a:", a, " b:", b, " c:", c, "\n")
    }
  }
  
  return(list(a = a, b = b, c = c, cost_history = cost_history))
}



```


```{r}
N <- 40
U0_list <- Main_Training_Base$U0  # Liste des valeurs initiales
y_observe_list <- do.call(rbind, lapply(Main_Training_Base$Un, function(x) unlist(x)))  # Matrice des observations
learning_rate <- 0.001
max_iter <- 1000

result <- sgd(y_observe_list, U0_list, N, learning_rate, max_iter)
```



```{r}
# Plage des paramètres
param_ranges <- list(a = c(min(params$a), max(params$a)), b = c(min(params$b), max(params$b)), c = c(min(params$c), max(params$c)))

# Fonction pour initialiser aléatoirement les paramètres
random_init <- function(param_ranges) {
  list(
    a = runif(1, param_ranges$a[1], param_ranges$a[2]),
    b = runif(1, param_ranges$b[1], param_ranges$b[2]),
    c = runif(1, param_ranges$c[1], param_ranges$c[2])
  )
}

# Stocker les résultats des 5 tests
results <- list()

for (test in 1:5) {
  cat("\n--- Test", test, "---\n")
  
  # Initialisation aléatoire
  init_params <- random_init(param_ranges)
  
  # Ajuster les valeurs initiales dans la fonction
  result <- sgd(
    y_observe_list = y_observe_list,
    U0_list = U0_list,
    N = N,
    learning_rate = learning_rate,
    max_iter = max_iter
  )
  
  # Stocker les résultats
  results[[test]] <- list(
    a_final = result$a,
    b_final = result$b,
    c_final = result$c,
    cost_history = result$cost_history
  )
}

```



```{r}
# Extraire les paramètres finaux pour chaque test
a_vals <- sapply(results, function(res) res$a_final)
b_vals <- sapply(results, function(res) res$b_final)
c_vals <- sapply(results, function(res) res$c_final)

# Fonction pour calculer les métriques
compute_metrics <- function(values) {
  mean_val <- mean(values)  # Moyenne
  variance <- var(values)   # Variance
  std_dev <- sd(values)     # Écart-type
  
  variance_relative <- variance / mean_val
  std_dev_relative <- std_dev / mean_val
  
  list(
    mean = round(mean_val,2),
    variance_relative = variance_relative,
    std_dev_relative = std_dev_relative
  )
}

# Calculer les métriques pour chaque paramètre
metrics_a <- compute_metrics(a_vals)
metrics_b <- compute_metrics(b_vals)
metrics_c <- compute_metrics(c_vals)

```


```{r}
cbind(metrics_a,metrics_b,metrics_c)
```

### compute the accuracy of the model 
#### The training dataset


```{r}
a_sgd<-2.09
b_sgd<-1.09
c_sgd<-0.28
out_size<-40

# Exemple d'utilisation
# Matrices de valeurs réelles et prédites (par exemple 3 observations avec 2 variables)
y_real_data_sgd <- do.call(rbind, lapply(Main_Training_Base$Un, function(x) unlist(x)))
y_pred_data_sgd <- t(mapply( Generate_output ,out_size,a_sgd, b_sgd ,c_sgd, Main_Training_Base$U0))

# Calcul du MAE global
mae_train_sgd <- calculate_mae_per_row(y_real_data_sgd, y_pred_data_sgd)

# Affichage du résultat
print(paste("Le MAE Train sgd est :", mae_train_sgd))
```

### The test dataset ;
```{r}
# Exemple d'utilisation
# Matrices de valeurs réelles et prédites (par exemple 3 observations avec 2 variables)
y_real_test_sgd <- do.call(rbind, lapply(Main_Test_Base$Un, function(x) unlist(x)))
y_pred_test_sgd <- t(mapply( Generate_output ,out_size,a_sgd, b_sgd ,c_sgd, Main_Test_Base$U0))

# Calcul du MAE global
mae_test_sgd <- calculate_mae_per_row(y_real_test_sgd, y_pred_test_sgd)

# Affichage du résultat
print(paste("Le MAE Test sgd est :", mae_test_sgd))
```

### Comparaison entre l'algorithme DIRECT et l'algorithme basé sur la méthode de descente de gradient (SGD)

#### **DIRECT**
L'algorithme **DIRECT** est un algorithme d'optimisation global qui explore systématiquement l'espace de recherche. Il est généralement plus robuste pour trouver des solutions optimales, indépendamment des conditions initiales, mais peut être plus coûteux en termes de calculs.

#### **SGD (Stochastic Gradient Descent)**
L'algorithme **SGD**, en revanche, utilise des mises à jour incrémentales basées sur le gradient. Il est plus rapide, mais peut être sensible aux choix des paramètres (comme le taux d'apprentissage) et peut se coincer dans des minima locaux.

### **Comparaison des résultats**
- **DIRECT** donne un meilleur résultat sur les données de test et d'entraînement avec un **MAE plus faible** :
  - **MAE Train (DIRECT)** : 6.22
  - **MAE Test (DIRECT)** : 6.45
- **SGD** a un **MAE plus élevé** :
  - **MAE Train (SGD)** : 22.19
  - **MAE Test (SGD)** : 20.82

### **Conclusion**
- **DIRECT** semble plus approprié dans ce cas, car il donne de meilleurs résultats, en particulier pour des espaces de recherche complexes où les minima locaux peuvent poser problème.
- **SGD** serait plus adapté pour des problèmes à grande échelle ou lorsqu'une bonne initialisation et un réglage précis du taux d'apprentissage sont possibles.


# 4 Machine Learning approach
## 4.1 Principle of the approach

### 1. **Principe global du Machine Learning**
Le **Machine Learning** (apprentissage automatique) consiste à utiliser des algorithmes pour permettre à un modèle de **s'apprendre** des **données** et d'améliorer ses performances sans être explicitement programmé. L'idée est d'entraîner un modèle avec des données d'exemples (données d'apprentissage) pour qu'il puisse ensuite faire des prédictions ou des classifications sur de nouvelles données. Il existe trois types principaux d'apprentissage : supervisé, non supervisé et par renforcement.

### 2. **Intérêts et limites par rapport au couplage de modèles de données**
**Intérêts du Machine Learning** :
- **Adaptabilité** : Le Machine Learning peut s'adapter à des problèmes complexes sans nécessiter une compréhension complète des relations sous-jacentes.
- **Automatisation** : Il permet d'automatiser la prise de décision en traitant de grandes quantités de données.

**Limites par rapport au couplage de modèles** :
- **Interprétabilité limitée** : Les modèles de Machine Learning, surtout les modèles complexes comme les réseaux de neurones, sont souvent des boîtes noires, difficiles à interpréter.
- **Nécessité de données** : Ces modèles nécessitent de grandes quantités de données annotées pour entraîner les modèles efficacement.
- **Manque de connaissance physique** : Le Machine Learning peut ne pas prendre en compte les lois physiques ou les connaissances spécifiques du domaine, contrairement au **DataModel Coupling**, qui modélise les relations physiques et peut être plus précis dans des contextes bien compris.

### 3. **Fonctionnement d'un algorithme de réseau de neurones**
Un **réseau de neurones** est une structure inspirée du cerveau humain qui comprend des **neurones artificiels** organisés en couches. Chaque neurone dans une couche prend des entrées, effectue une transformation (souvent une somme pondérée suivie d'une activation), et transmet le résultat à la couche suivante. L'algorithme ajuste les poids des connexions entre neurones pendant l'entraînement à l'aide d'une technique comme la descente de gradient, en cherchant à minimiser l'erreur entre les prédictions du modèle et les valeurs réelles.

### 4. **Influence du nombre de neurones et de couches sur les résultats**
- **Nombre de neurones** : Un nombre plus élevé de neurones dans chaque couche permet au réseau de capturer des relations plus complexes dans les données, mais peut aussi mener à un **sur-apprentissage** si le réseau devient trop complexe par rapport à la quantité de données.
- **Nombre de couches** : Un plus grand nombre de couches (réseau profond) permet au modèle de capturer des relations de plus haut niveau et plus abstraites. Cependant, cela peut également rendre l'entraînement plus difficile, nécessitant des techniques avancées pour éviter le **problème de vanishing gradient** et garantir une bonne généralisation.


### Build Data

```{r}
ML_data<-head(Main_Learning_Base,-3)
Learn_data <- ML_data%>%
  # Décomposer la liste Un en plusieurs lignes
  unnest(Un) %>%
  # Ajouter une nouvelle colonne avec le numéro de courbe
  mutate(Ncurve = rep(1:nrow(ML_data), times = sapply(ML_data$Un, length))) %>%
  # Ajouter l'indice n pour chaque élément dans Un
  group_by(Ncurve) %>%
  mutate(n = row_number()) %>%
  ungroup() %>%
  select(Ncurve, n, U0, TS = Un)
```


```{r}
Ntrain<-0.3*100*41
# Construire Train_Base_1 avec les 30% premières lignes
Train_Base_1 <- Learn_data[1:Ntrain, ]
Test_Base_1<-Learn_data[(Ntrain + 1):nrow(Learn_data) , ]
```



Le code de la question 3 crée une fonction de __normalisation__ pour les ensembles de données d'entraînement et de test __(TrainB et TestB)__. Il normalise les colonnes 2 à 4 de ces jeux de données en les __centrant__ et en les __réduisant__ par rapport à la moyenne et à l'étendue des données d'entraînement. Ensuite, les données normalisées sont extraites et les noms des colonnes sont définis pour les matrices résultantes.


## Reseau neurone

Le modèle que nous allons utiliser dans dans le code est un réseau de neurones artificiels (RNA) pour une tâche de régression.


```{r}
Train_Base_1_S <- Train_Base_1
Train_Base_1_S[, c("n", "U0", "TS")] <- scale(Train_Base_1_S[, c("n", "U0", "TS")])
Test_Base_1_S <- Test_Base_1
Test_Base_1_S[, c("n", "U0", "TS")] <- scale(Test_Base_1_S[, c("n", "U0", "TS")])
```

```{r}
library(neuralnet)

# Assurez-vous que les données sont standardisées (vous l'avez déjà fait avec scale)
set.seed(333)

nc <- neuralnet(TS ~ n + U0,
                data = Train_Base_1_S,
                hidden = c(3, 6, 3),
                err.fct = "sse",
                linear.output = FALSE,
                lifesign = 'full',
                rep = 10,
                algorithm = "rprop+",
                stepmax = 700,
                thresh = 0.001)  # Réduire le seuil de convergence

# Résumé du modèle entraîné
summary(nc)

```



Entrée (n, U0)
        |
  [Couche cachée 1] (3 neurones)
        |
  [Couche cachée 2] (6 neurones)
        |
  [Couche cachée 3] (3 neurones)
        |
   Sortie (TS)



### Modèle de Réseau de Neurones : 3 Couches Cachées (3, 6, 3 Neurones)

Le modèle ajuste un total de **58 paramètres**, répartis comme suit :

#### Détails des Poids et Biais
- **Poids** :
  - 6 (entrée → 1ère couche)
  - 18 (1ère → 2ème couche)
  - 18 (2ème → 3ème couche)
  - 3 (3ème → sortie)
  - **Total des poids : 45**

- **Biais** :
  - 3 (1ère couche)
  - 6 (2ème couche)
  - 3 (3ème couche)
  - 1 (sortie)
  - **Total des biais : 13**

#### Total des Paramètres
- **45 Poids + 13 Biais = 58 Paramètres**

---

### Comparaison avec l'Approche Modèle-Données (Régression Linéaire)

Dans un modèle de régression linéaire avec 2 variables explicatives (`n` et `U0`), le nombre de paramètres ajustés est beaucoup plus faible :
- **Régression Linéaire** :
  - 2 coefficients (pour `n` et `U0`)
  - 1 biais (interception)
  - **Total : 3 paramètres**

---

### Conclusion

- Le **modèle de réseau de neurones** ajuste **58 paramètres**, ce qui est **beaucoup plus complexe** qu'une **régression linéaire** qui ajuste seulement **3 paramètres** ou la descente gradient.
- Cette différence illustre la **flexibilité** et la **puissance** des réseaux de neurones pour capturer des relations non linéaires, mais aussi leur **complexité accrue**.


## Exercise 4.3 Study of the fitted model


```{r}
library(ggplot2)
t
Train_Base_1_S$Predicted_TS <- predict(nc, newdata = Train_Base_1_S)
Test_Base_1_S$Predicted_TS <- predict(nc, newdata = Test_Base_1_S)

# Fonction pour tracer une courbe donnée
plot_curve <- function(data, curve_id, title) {
  data_filtered <- data[data$Ncurve == curve_id, ]
  
  ggplot(data_filtered, aes(x = n)) +
    geom_line(aes(y = TS, color = "Initial Curve"), size = 1) +
    geom_line(aes(y = Predicted_TS, color = "Predicted Curve"), linetype = "dashed", size = 1) +
    labs(title = title, x = "n", y = "TS", color = "Legend") +
    theme_minimal()
}

# Sélection de quelques courbes pour l'ensemble d'entraînement
train_curves <- unique(Train_Base_1_S$Ncurve)[1:3]
plots_train <- lapply(train_curves, function(curve_id) {
  plot_curve(Train_Base_1_S, curve_id, paste("Train Curve ID:", curve_id))
})

# Sélection de quelques courbes pour l'ensemble de test
test_curves <- unique(Test_Base_1_S$Ncurve)[1:3]
plots_test <- lapply(test_curves, function(curve_id) {
  plot_curve(Test_Base_1_S, curve_id, paste("Test Curve ID:", curve_id))
})

# Affichage des graphiques
library(gridExtra)
grid.arrange(grobs = plots_train, ncol = 2, top = "Training Curves")
grid.arrange(grobs = plots_test, ncol = 2, top = "Test Curves")

```



