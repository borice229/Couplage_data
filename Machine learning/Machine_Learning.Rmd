---
title: "Machine Learning et ADC"
author: "Borice DOSSOU"
date: "2024-09-04"
output:
  html_document: default
  pdf_document: default
---

```{r}
rm(list = ls())
```

# Chargement des library
```{r}
library(ggplot2)
library(tidyr)
library(tidyverse)
library(reshape2)
library(dplyr)
library(DBI)
library(RMySQL)
library(jsonlite)
```


Fonction generatrice de la suite
```{r}
# Définition de la fonction
Generate_output <- function(N, a, b, c, U0) {
  # Initialisation U0
  U <- numeric(N + 1)
  U[1] <- U0
  
  # Boucle pour calculer les valeurs de U_n
  for (n in 1:N) {
    U[n + 1] <- a * U[n]^c + b
  }
  
  return(U)
}
```


La fonction de test de function
_Essai 1_ 

```{r}
test_Generate_output <- function(){
 a <- 2
 b <- 2
 c <- 1
 U0 <- 1
 N <- 2
 waited<-c(2,2,2,1,1,4,10)
 fun_out<-c(N,a,b,c,Generate_output(N,a,b,c,U0))
 if(all(waited==fun_out)){
   return("OK")
 }else{
   return("Error...")
   }
}
test_Generate_output()
```

__Essai 2__

```{r}
test_Generate_output <- function(){
 a <- 2
 b <- 2
 c <- 1
 U0 <- 1
 N <- 1
 waited<-c(1,2,2,1,1,4)
 fun_out<-c(N,a,b,c,Generate_output(N,a,b,c,U0))
 if(all(waited==fun_out)){
   return("OK")
 }else{
   return("Error...")
   }
}
test_Generate_output()
```
Après avoir Exécuter le code de la fonction. Je constate que le code marche alors je peux le deployer.

Generer les courbes selons les valeurs des paramètres
Let N = 40, a ∈ [2; 5], b ∈ [1; 5] and , c ∈ [0.1; 0.9]

```{r}
# Pour rendre les résultats reproductibles (facultatif)
# Nombre d'itérations
N <- 40
U0 <- 2  # Valeur initiale de U(0)

# Paramètres pour la courbe 1
a1<-2
b1<-1
c1<-0.1
# Paramètres pour la courbe 2
a2<-5
b2<-5
c2<-0.9
# Paramètres pour la courbe 3
a3<-3.5
b3<-3
c3<-0.5
# Paramètres pour la courbe 4
a4 <- 4
b4 <- 2
c4 <- 0.3
# Paramètres pour la courbe 5
a5 <- 2.5
b5 <- 4
c5 <- 0.7
# Paramètres pour la courbe 6
a6 <- 3
b6 <- 1.5
c6 <- 0.4

# Pour permettre un affichage en grille des graphique
par(mfrow = c(3, 2))
# Générer et tracer la courbe 1
plot(0:N, Generate_output(N, a1,b1,c1, U0),
     type = "o", col = 1, ylim = c(0, 4), 
     xlab = "n", ylab = "U_n", 
     main = "Courbe de a=2, b=1, c=0.1")
# Générer et tracer la courbe 2
plot(0:N, Generate_output(N, a2,b2,c2, U0),
     type = "o", col = 1, ylim = c(0, 10^7), 
     xlab = "n", ylab = "U_n", 
     main = "Courbe de a=5, b=5, c=0.9")
# Générer et tracer la courbe 3
plot(0:N, Generate_output(N, a3,b3,c3, U0),
     type = "o", col = 1, ylim = c(0, 20), 
     xlab = "n", ylab = "U_n", 
     main = "Courbe de a=3.5, b=3, c=0.5")
# Générer et tracer la courbe 4
plot(0:N, Generate_output(N, a4, b4, c4, U0),
     type = "o", col = 1, ylim = c(0, 15), 
     xlab = "n", ylab = "U_n", 
     main = "Courbe de a=4, b=2, c=0.3")

# Générer et tracer la courbe 5
plot(0:N, Generate_output(N, a5, b5, c5, U0),
     type = "o", col = 1, ylim = c(0, 40), 
     xlab = "n", ylab = "U_n", 
     main = "Courbe de a=2.5, b=4, c=0.7")

# Générer et tracer la courbe 6
plot(0:N, Generate_output(N, a6, b6, c6, U0),
     type = "o", col = 1, ylim = c(0, 10), 
     xlab = "n", ylab = "U_n", 
     main = "Courbe de a=3, b=1.5, c=0.4")

```


Description des paramètres

a: contrôle le taux de croissance,
b: agit comme un décalage vertical,
c: modifie la non-linéarité de la croissance


__Épidémiologie__
Propagation des Maladies : Ce modèle peut également être utilisé pour modéliser la propagation des maladies infectieuses où le taux de propagation est influencé de manière non linéaire par le nombre d'individus infectés

__Dynamique des Populations__
Croissance des Populations : Ce modèle peut être utilisé pour modéliser la croissance des populations biologiques. 



__Exercise 2.2 Behavior of the parameters of the model__

1 ) La sensibilité des paramètres fait référence à la manière dont de petites variations des paramètres d'un modèle ou d'une équation affectent le comportement ou les résultats de ce modèle. En d'autres termes, si un paramètre change légèrement, la sensibilité mesure l'ampleur de la variation dans la sortie du modèle. Un système est dit sensible à un paramètre si une petite modification de ce dernier entraîne un changement significatif dans les résultats, tandis qu'il est insensible ou robuste si ces variations ont peu d'effet. La sensibilité est importante pour comprendre la stabilité d'un modèle et identifier les paramètres critiques qui influencent fortement le système."""

2) 


Approche : Fixer b et c à des valeurs constantes, puis augmenter ou diminuer progressivement a (par exemple, tester des valeurs dans l'intervalle [2,5].
Observer comment la vitesse de croissance de la suite change.
Sensibilité : Si de petites variations de a entraînent de grandes différences dans la croissance ou la divergence de la suite, cela indique une grande sensibilité au paramètrea.


```{r}
# Générer les valeurs des paramètres à partir des intervalles
a_values <- seq(from = 2, to = 5, length.out = 5)
b_values <- seq(from = 1, to = 5, length.out = 5)
c_values <- seq(from = 0.1, to = 0.9, length.out = 5)
U0_valeus<-seq(from = 1, to = 5, length.out = 5)
# Créer une grille de combinaisons possibles des paramètres
param_grid <- expand.grid(a = a_values, b = b_values, c = c_values, U0=U0_valeus)
param_grid
```



```{r}
calculate_gradient <- function(N, a, b, c, U0) {
  h_values <- seq(0.001, 0.05, length.out = 10) # Définir une plage de h
  fU <- Generate_output(N, a, b, c, U0)
  
  # Matrices pour stocker les variations pour chaque h
  variations_a <- matrix(0, nrow = length(fU), ncol = length(h_values))
  variations_b <- matrix(0, nrow = length(fU), ncol = length(h_values))
  variations_c <- matrix(0, nrow = length(fU), ncol = length(h_values))
  
  # Calcul des variations pour chaque h
  for (i in seq_along(h_values)) {
    h <- h_values[i]
    variations_a[, i] <- (Generate_output(N, a + h, b, c, U0) - fU) / h
    variations_b[, i] <- (Generate_output(N, a, b + h, c, U0) - fU) / h
    variations_c[, i] <- (Generate_output(N, a, b, c + h, U0) - fU) / h
  }
  
  # Calcul de la moyenne des variations pour chaque paramètre
  gradient_a <- rowMeans(variations_a)
  gradient_b <- rowMeans(variations_b)
  gradient_c <- rowMeans(variations_c)
  
  # Combiner les résultats en une matrice
  gradient_f <- round(cbind(gradient_a, gradient_b, gradient_c, U0),2)
  
  return(gradient_f)
}

```



```{r}
test_calculate_gradient <- function() {
  # Paramètres pour le test
  N <- 3
  a <- 2
  b <- 2
  c <- 1
  U0 <- 1
  
  # Résultat attendu (valeurs connues ou calculées manuellement)
  waited <- c(
    0.00, 0.00, 0.00, 1,  # Ligne 1
    1.00, 1.00, 0.00, 1,  # Ligne 2
    6.03, 3.00, 11.29, 1, # Ligne 3
    22.20, 7.00, 72.05, 1 # Ligne 4
  )
  
  # Résultat obtenu
  gradients <- calculate_gradient(N, a, b, c, U0)
  fun_out <- as.vector(t(gradients)) # Transformer la matrice en vecteur ligne
  
  # Comparaison des résultats
  if (all.equal(waited, fun_out, tolerance = 1e-2)) {
    return("OK")
  } else {
    return("Error...")
  }
}

# Exécuter le test
test_calculate_gradient()

```


```{r}
# Calculer la norme du gradient
calculate_gradient_norm <- function(gradients) {
  # Calcul de la norme pour chaque ligne
  norms <- apply(gradients[,1:3], 1, function(row) sqrt(sum(row^2)))
  
  return(norms)
}
```




```{r}
test_calculate_gradient_norm <- function() {
  # Gradients d'entrée pour le test (matrice)
  gradients <- matrix(
    c(
      0.00, 0.00, 0.00, 1,  # Ligne 1
      1.00, 1.00, 0.00, 1,  # Ligne 2
      6.03, 3.00, 11.29, 1, # Ligne 3
      22.20, 7.00, 72.05, 1 # Ligne 4
    ),
    ncol = 4,
    byrow = TRUE
  )
  
  # Résultat attendu (valeurs calculées manuellement)
  waited <- c(
    0.00,                     # Norme de la ligne 1
    sqrt(1.00^2 + 1.00^2),    # Norme de la ligne 2
    sqrt(6.03^2 + 3.00^2 + 11.29^2), # Norme de la ligne 3
    sqrt(22.20^2 + 7.00^2 + 72.05^2) # Norme de la ligne 4
  )
  
  # Résultat obtenu
  fun_out <- calculate_gradient_norm(gradients)
  
  # Comparaison des résultats
  if (all.equal(waited, fun_out, tolerance = 1e-2)) {
    return("OK")
  } else {
    return("Error...")
  }
}

# Exécuter le test
test_calculate_gradient_norm()

```


```{r}
calculate_sensitivity_indicateur <- function(gradients) {
  # Vérifier si la matrice des gradients est valide
  if (is.null(gradients) || nrow(gradients) == 0 || ncol(gradients) < 3) {
    stop("La matrice des gradients doit contenir au moins 3 colonnes et une ou plusieurs lignes.")
  }
  
  # Calculer la norme des gradients pour chaque ligne
  norms <- calculate_gradient_norm(gradients)
  
  # Éviter les divisions par zéro en ajoutant une petite valeur epsilon
  epsilon <- 1e-6
  sensitivity_values <- rowSums(gradients[, 1:3]) / (norms + epsilon)
  
  # Calculer l'indicateur global en prenant la médiane des sensibilités
  median_indicator <- round(median(sensitivity_values, na.rm = TRUE), 2)
  
  return(median_indicator)
}


```



```{r}
test_calculate_sensitivity_indicateur <- function() {
  # Définir un exemple de gradients (entrée)
  test_gradients <- matrix(c(
    1, 2, 3,   # Ligne 1
    4, 5, 6,   # Ligne 2
    7, 8, 9    # Ligne 3
  ), ncol = 3, byrow = TRUE)
  
  # Résultat attendu pour cet exemple
  # Normes : sqrt(1^2 + 2^2 + 3^2), sqrt(4^2 + 5^2 + 6^2), sqrt(7^2 + 8^2 + 9^2)
  # Sensibilités relatives : rowSums(gradients) / norms
  # Indicateur : médiane des sensibilités relatives
  expected_indicator <- round(median(c(
    sum(c(1, 2, 3)) / sqrt(sum(c(1, 2, 3)^2)), # Ligne 1
    sum(c(4, 5, 6)) / sqrt(sum(c(4, 5, 6)^2)), # Ligne 2
    sum(c(7, 8, 9)) / sqrt(sum(c(7, 8, 9)^2))  # Ligne 3
  )), 2)
  
  # Appeler la fonction avec l'entrée
  result_indicator <- calculate_sensitivity_indicateur(test_gradients)
  
  # Vérifier le résultat
  if (all.equal(result_indicator, expected_indicator, tolerance = 1e-2)) {
    return("OK")
  } else {
    return(paste("Test Échec: Attendu =", expected_indicator, "Obtenu =", result_indicator))
  }
}
test_calculate_sensitivity_indicateur()
```


Stock des Indicateur
```{r}
Stock_sensitivity_params <- function(param_list, N) {
  # Initialiser une liste vide pour stocker les résultats
  stock <- data.frame(a = numeric(), b = numeric(), c = numeric(), U0 = numeric(), Indicator = numeric())
  
  # Parcourir chaque ensemble de paramètres dans la liste
  for (params in param_list) {
    a <- params$a
    b <- params$b
    c <- params$c
    U0 <- params$U0
    
    # Calculer les gradients pour cet ensemble de paramètres
    gradients <- calculate_gradient(N, a, b, c, U0)
    
    # Calculer l'indicateur de sensibilité pour les gradients obtenus
    indicator <- round(calculate_sensitivity_indicateur(gradients),2)
    
    # Ajouter les résultats à la base stock
    stock <- rbind(stock, data.frame(a = a, b = b, c = c, U0 = U0, Indicator = indicator))
  }
  
  return(stock)
}

```




```{r}
get_sensibility <- function(stock) {
  # Calculer la médiane de la colonne Indicator
  median_value <- median(stock$Indicator, na.rm = TRUE)
  
  # Trouver l'indice de la ligne dont la valeur d'Indicator est la plus proche de la médiane
  closest_index <- which.min(abs(stock$Indicator - median_value))
  
  # Retourner la ligne correspondante
  selected_row <- stock[closest_index, ]
  
  return(selected_row)
}

```


```{r}
test_get_sensibility <- function() {
  # Créer un dataframe de test avec des valeurs prédéfinies
  stock <- data.frame(
    a = c(1, 2, 3, 4, 5),
    b = c(10, 15, 20, 25, 30),
    c = c(0.5, 1, 1.5, 2, 2.5),
    U0 = c(1, 2, 3, 4, 5),
    Indicator = c(2, 7, 12, 3, 9)  # Valeurs arbitraires
  )
  
  # Résultat attendu : médiane = 7, la ligne la plus proche de 7 est la 2ème (Indice 2)
  expected_result <- stock[2, ]
  
  # Appeler la fonction get_sensibility
  result <- get_sensibility(stock)
  
  # Vérification si le résultat correspond à ce qui est attendu
  if (all(result == expected_result)) {
    return("ok")
  } else {
    return("Erreur : Résultat incorrect")
  }
}

# Exécution du test
test_get_sensibility()

```


```{r}
# Exemple d'utilisation : définir plusieurs ensembles de paramètres
param_list <- list(
  list(a = 2, b = 2.5, c = 0.4, U0 = 1),
  list(a = 3.0, b = 4.0, c = 0.6, U0 = 1),
  list(a = 2.0, b = 3.0, c = 0.5, U0 = 1),
  list(a = 5.0, b = 5.0, c = 0.7, U0 = 1)
)

# Trouver les paramètres avec la meilleure sensibilité
Param_sim <- get_sensibility(Stock_sensitivity_params(param_list, N = 100))
print(Param_sim)

```

__Exercise 2.3 Distribution of the parameters__


Les paramètres d'entrée du modèle sont :

_a_ : Ce paramètre contrôle le facteur de croissance multiplicatif de la suite Un. Il influence directement la manière dont Un. Plus la valeur de a est élevée, plus la suite croît rapidement.

_b_ : Ce paramètre représente un terme constant qui est ajouté à chaque itération. Il détermine le biais ou la contribution fixe à chaque étape de la suite. Un b élevé entraîne.

_c_ : Ce paramètre contrôle la puissance à laquelle Un est élevé. Il modifie la manière non linéaire dont un influence sa propre évolution. Un c plus grand accentue les écarts, tandis qu'un c plus petit atténue la croissance de Un

_U0_  : La condition initiale, qui définit la valeur de départ de la suite U. C'est la première valeur qui déclenche l'évolution de la suite selon les paramètresa,b et c

_N_ : Le nombre d'itérations ou d'étapes, représentant la durée sur laquelle la suite est calculée.

_Sorties_ :
Les sorties du modèle sont : _Un_: La suite de valeurs obtenue après N itérations, où chaque valeur  Un dépend des paramètres a,b et c
ainsi que de la valeur précédente Un−1. C'est la sortie principale du modèle qui décrit l'évolution de la suite en fonction des entrées.



__Lois des paramètres a,b et c.__
Je simule une loi normale  pour mes paramètre
Mon param


```{r}
set.seed(123) # Pour des résultats reproductibles
n <- 50 # Nombre de simulations souhaité

# Simulation à partir des lois normales
sim_a <- rnorm(n, mean = 3, sd = 0.2)
sim_b <- rnorm(n, mean = 4, sd = 0.3)
sim_c <- rnorm(n, mean = 0.6, sd = 0.05)

sim_U0 <- round(runif(5, min = 1, max = 10),0)

# Créer les histogrammes
par(mfrow = c(1, 3))  # Pour afficher les 3 histogrammes côte à côte

hist(sim_a, main = "Histogramme de a", xlab = "a", col = "lightblue", breaks = 30)
hist(sim_b, main = "Histogramme de b", xlab = "b", col = "lightgreen", breaks = 30)
hist(sim_c, main = "Histogramme de c", xlab = "c", col = "lightcoral", breaks = 30)

```



__Exercise 3.1 The notion of noisy data__

__1. Notion de bruit dans la collecte de données__
Le bruit en collecte de données désigne les variations aléatoires ou les informations non pertinentes qui perturbent les données recueillies. Ces perturbations ne reflètent pas le phénomène réel mesuré. Le bruit peut provenir de sources variées, telles que des erreurs de mesure, des variations environnementales ou des biais dans les instruments de collecte.

__2. Problèmes causés par le bruit__
La présence de bruit peut entraîner plusieurs problèmes :

__Baisse de la précision__ : Le bruit rend les données moins fiables et rend difficile la distinction entre les variations aléatoires et les tendances réelles.
Difficulté à interpréter : Le bruit complique l'analyse, car il peut masquer les relations importantes entre les variables.
Modèles biaisés : En présence de bruit important, les modèles statistiques peuvent produire des résultats inexacts ou non représentatifs.

__3. Loi de probabilité du composant aléatoire__
Le composant aléatoire, c'est-à-dire le bruit, peut souvent être modélisé par une loi normale (gaussienne), car les erreurs et perturbations aléatoires ont tendance à suivre une distribution symétrique autour de zéro avec une variance spécifique. Cependant, d'autres distributions peuvent être utilisées selon la nature du bruit (loi uniforme pour des erreurs constantes, loi exponentielle pour des erreurs asymétriques, etc.). La loi normale est souvent privilégiée car elle reflète bien les erreurs aléatoires dans de nombreux contextes.

__Exercise 3.2 Generation of a learning database__

```{r}
# Simulation des paramètres pour n individus
set.seed(123)  # Pour la reproductibilité
M <- 100  # Nombre d'individus
N <- 40  # Nombre d'itérations

# Simulation des paramètres a, b, c, et U0
sim_a <- round(rnorm(M, mean = 3, sd = 0.2), 2)  # Simulation des paramètres a
sim_b <- round(rnorm(M, mean = 4, sd = 0.3), 2)  # Simulation des paramètres b
sim_c <- round(rnorm(M, mean = 0.6, sd = 0.05), 2)  # Simulation des paramètres c
sim_Uo <- round(runif(M, min = 1, max = 10), 2)  # Simulation des valeurs initiales de U0

# Générer une base de données avec les valeurs de a, b, c et U[0], U[1], ..., U[N]
base_root <- data.frame(matrix(ncol = N + 4, nrow = M))  # Base vide (n lignes, N+4 colonnes)

# Noms des colonnes : a, b, c, U[0], ..., U[100]
colnames(base_root) <- c("a", "b", "c", paste0("U", 0:N))

# Remplir la base avec les valeurs simulées de a, b, c et les valeurs de U
for (i in 1:M) {
  base_root[i, 1] <- sim_a[i]  # Valeur de a
  base_root[i, 2] <- sim_b[i]  # Valeur de b
  base_root[i, 3] <- sim_c[i]  # Valeur de c
  base_root[i, 4] <- sim_Uo[i]  # Valeur de Uo
  
  # Calculer les valeurs de U à l'aide de la fonction Generate_output (assurez-vous qu'elle est définie)
  U_values <- round(Generate_output(N, sim_a[i], sim_b[i], sim_c[i], sim_Uo[i]), 2)
  
  # Ajouter les valeurs de U dans la base
  base_root[i, 4:(N + 4)] <- U_values
}

# Restructurer les données pour le graphique
base_root %>%
  mutate(Individu = 1:M) %>%               # Ajouter la colonne 'Individu'
  melt(id.vars = c("a", "b", "c", "Individu"), 
       variable.name = "Iteration", 
       value.name = "U") %>%                # Restructurer le dataframe
  mutate(Iteration = as.numeric(gsub("U", "", Iteration))) %>%  # Convertir la colonne 'Iteration' en numérique
  ggplot(aes(x = Iteration, y = U, color = factor(Individu))) + # Graphique ggplot
  geom_line(linewidth = 1) +                     # Remplacer 'size' par 'linewidth'
  labs(title = "Évolution de U[n] pour chaque individu",
       x = "Itération (N)", 
       y = "Valeur de U[N]",
       color = "Individu") +
  theme_minimal()

```



```{r}
set.seed(123)  # Pour la reproductibilité
# Générer une base de données avec les valeurs de a, b, c et U[0], U[1], ..., U[100]
base_brutee <- data.frame(matrix(ncol = N + 4, nrow = M))  # Base vide (n lignes, N+4 colonnes)

# Noms des colonnes : a, b, c, U[0], ..., U[100]
colnames(base_brutee) <- c("a", "b", "c", paste0("U", 0:N))

# Remplir la base avec les valeurs simulées de a, b, c et les valeurs de U
for (i in 1:M) {
  base_brutee[i, 1] <- sim_a[i]  # Valeur de a
  base_brutee[i, 2] <- sim_b[i]  # Valeur de b
  base_brutee[i, 3] <- sim_c[i]  # Valeur de c
  base_brutee[i, 4] <- sim_Uo[i]  # Valeur de Uo
  U_values_bruité <- Generate_output(N, sim_a[i], sim_b[i], sim_c[i], sim_Uo[i]) + rnorm(N+1, mean = 0, sd = 1) # Générer   
  base_brutee[i, 4:(N + 4)] <- round(U_values_bruité,2)  # Remplir les colonnes U[0] à U[100]
}

base_brutee %>%
  mutate(Individu = 1:M) %>%               # Ajouter la colonne 'Individu'
  melt(id.vars = c("a", "b", "c", "Individu"), 
       variable.name = "Iteration", 
       value.name = "U") %>%                # Restructurer le dataframe
  mutate(Iteration = as.numeric(gsub("U", "", Iteration))) %>%  # Convertir la colonne 'Iteration' en numérique
  ggplot(aes(x = Iteration, y = U, color = factor(Individu))) + # Graphique ggplot
  geom_line(size = 1) +                     # Tracer les courbes pour chaque individu
  labs(title = "Évolution de U[n] bruité pour chaque individu",
       x = "Itération (N)", 
       y = "Valeur de U[N]",
       color = "Individu") +
  theme_minimal()    
```

```{r}
set.seed(123)  # Pour la reproductibilité
# Nombre de simulations
M <- 100 # Ajustez ce nombre selon vos besoins
N <- 40   # Nombre d'itérations pour Generate_output

# Simulation des paramètres
sim_a <- round(rnorm(M, mean = 3, sd = 0.2), 2)  # Simulation des paramètres a
sim_b <- round(rnorm(M, mean = 4, sd = 0.3), 2)  # Simulation des paramètres b
sim_c <- round(rnorm(M, mean = 0.6, sd = 0.05), 2)  # Simulation des paramètres c
sim_U0 <- round(runif(M, min = 1, max = 10), 2)  # Simulation des valeurs initiales de U0
```


```{r}
set.seed(123)  # Pour la reproductibilité

# Génération des résultats et création de la DataFrame
Output_Curves <- data.frame(
  a = sim_a,
  b = sim_b,
  c = sim_c,
  U0 = sim_U0
)

# Calcul des sorties Un et ajout comme une colonne
Output_Curves$Un <- lapply(1:nrow(Output_Curves), function(i) {
  Generate_output(N, Output_Curves$a[i], Output_Curves$b[i], Output_Curves$c[i], Output_Curves$U0[i])
})


# Affichage de la DataFrame
head(Output_Curves)

```



```{r}
# Génération des résultats et création de la DataFrame
Main_Learning_Base <- data.frame(
  a = sim_a,
  b = sim_b,
  c = sim_c,
  U0 = sim_U0
)

# Calcul des sorties Un et ajout comme une colonne
Main_Learning_Base$Un <- lapply(1:nrow(Main_Learning_Base), function(i) {
  # Générer les valeurs de Un avec Generate_output
  generated_values <- Generate_output(N, Main_Learning_Base$a[i], Main_Learning_Base$b[i], Main_Learning_Base$c[i], Main_Learning_Base$U0[i])
  
  generated_values[-1] <- generated_values[-1] + rnorm(N, mean = 0, sd = 1)
  
  return(generated_values)
})

```


# Build SQL Base


```{r}
# Établir la connexion
con <- dbConnect(RMySQL::MySQL(), 
                 dbname = "Mlearn",  # Nom de la base de données
                 host = "127.0.0.1",    # Adresse locale du serveur MySQL
                 port = 3306,           # Port par défaut pour MySQL
                 user = "root",         # Nom d'utilisateur
                 password = "less@intsARS19") # Mot de passe réel

# Vérification de la connexion
if (!is.null(con)) {
  cat("Connexion réussie à la base de données 'mlearning' !\n")
} else {
  cat("Erreur lors de la connexion.\n")
}

```



# table des paramètres

```{r}
param<-Main_Learning_Base[, 1:4]
```


```{r}
# Créer la table Parameters
dbExecute(con, "
  CREATE TABLE IF NOT EXISTS Parameters (
    id INT AUTO_INCREMENT PRIMARY KEY,
    a FLOAT NOT NULL,
    b FLOAT NOT NULL,
    c FLOAT NOT NULL,
    U0 FLOAT NOT NULL
    
  )
")

```
__Créer la table NormalData__

```{r}
# Créer la table NormalData avec l'ID comme clé primaire et la clé étrangère pour parameter_id
dbExecute(con, "
  CREATE TABLE IF NOT EXISTS NormalData (
    id INT AUTO_INCREMENT PRIMARY KEY,  -- Définir id comme clé primaire
    parameter_id INT NOT NULL,          -- Clé étrangère
    Un JSON NOT NULL,                   -- Colonne JSON pour stocker Un
    FOREIGN KEY (parameter_id) REFERENCES Parameters(id) ON DELETE CASCADE  -- Référence à Parameters
  )
")


```

__Créer la table NoiseData__

```{r}
# Créer la table NoiseData
dbExecute(con, "
  CREATE TABLE IF NOT EXISTS NoiseData (
    id INT AUTO_INCREMENT PRIMARY KEY,
    parameter_id INT NOT NULL,
    Un JSON NOT NULL,
    FOREIGN KEY (parameter_id) REFERENCES Parameters(id) ON DELETE CASCADE
  )
")


```


```{r}
# Désactiver les contraintes de clé étrangère
dbExecute(con, "SET foreign_key_checks = 0")

# Vider la table Parameters
dbExecute(con, "TRUNCATE TABLE Parameters")

# Réactiver les contraintes de clé étrangère
dbExecute(con, "SET foreign_key_checks = 1")


```
__Charger les données de la table Parameters__
```{r}
# Préparer les données à insérer, en excluant l'`id`
for (i in 1:nrow(param)) {
  query <- sprintf("INSERT INTO Parameters (a, b, c, U0) VALUES (%f, %f, %f, %f)",
                   param$a[i], param$b[i], param$c[i], param$U0[i])
  dbExecute(con, query)
}

```

__Charger les données de la table Output_Curves __
```{r}
# Créer la colonne 'parameter_id' qui contient des valeurs de 1 à N
Output_Curves$parameter_id <- 1:nrow(Output_Curves)

# Préparer les données à insérer dans la table NormalData
for (i in 1:nrow(Output_Curves)) {
  # Convertir la colonne Un en chaîne JSON
  Un_json <- toJSON(Output_Curves$Un[[i]], auto_unbox = TRUE)  # auto_unbox pour convertir en un tableau JSON correct
  
  # Construire la requête SQL
  query <- sprintf("INSERT INTO NormalData (parameter_id, Un) VALUES (%d, '%s')",
                   Output_Curves$parameter_id[i], Un_json)  # '%s' pour insérer la chaîne JSON

  # Exécuter la requête d'insertion
  dbExecute(con, query)
}

```



__Charger les données de la table Main_Learning_Base __
```{r}
# Créer la colonne 'parameter_id' qui contient des valeurs de 1 à N
Main_Learning_Base$parameter_id <- 1:nrow(Main_Learning_Base)

# Préparer les données à insérer dans la table NormalData
for (i in 1:nrow(Main_Learning_Base)) {
  # Convertir la colonne Un en chaîne JSON
  Un_j <- toJSON(Main_Learning_Base$Un[[i]], auto_unbox = TRUE)  # auto_unbox pour convertir en un tableau JSON correct
  
  # Construire la requête SQL
  query <- sprintf("INSERT INTO NoiseData (parameter_id, Un) VALUES (%d, '%s')",
                   Main_Learning_Base$parameter_id[i], Un_j)  # '%s' pour insérer la chaîne JSON

  # Exécuter la requête d'insertion
  dbExecute(con, query)
}
```







# Definir les base d'entrainement et de test

```{r}
# Supposons que Main_Learning_Base est déjà chargé et contient M lignes
set.seed(123)  # Pour la reproductibilité

# Déterminer le nombre de lignes pour chaque ensemble
train_size <- floor(0.7 * M)  # 70% pour l'entraînement
test_size <- M - train_size     # 30% pour le test

# Échantillonner les indices pour l'ensemble d'entraînement
train_indices <- sample(1:M, size = train_size, replace = FALSE)

# Créer les ensembles d'entraînement et de test
Main_Training_Base <- Main_Learning_Base[train_indices, ]  # Ensemble d'entraînement
Main_Test_Base <- Main_Learning_Base[-train_indices, ]      # Ensemble de test
```


Intérêt de cette approche :
L'approche vise à ajuster les paramètres d'un modèle (a,b,c) pour qu'il s'aligne au mieux avec des données réelles bruitées. Cela permet de trouver les paramètres qui minimisent la différence entre les prédictions du modèle et les observations. Ce processus est essentiel en modélisation pour :

Améliorer la précision des prédictions du modèle ;
Comprendre les relations sous-jacentes dans les données ;
Réduire l'erreur et ajuster le modèle aux particularités des données bruitées.



je veux ajouter d'autre ligne pour repondre à la fonction avec All_Uo

```{r}

Uo <- c(8.21, 1.572, 3.197)
a1<-1.72
b1<-2.72
c1<-0.80

# Vérifier et mettre à jour la base
for (u0 in Uo) {
  # Vérifier si les paramètres (a, b, c, U0) existent déjà dans `param`
  existing_row <- param[param$a == a1 & param$b == b1 & param$c == c1 & param$U0 == u0, ]
  
  if (nrow(existing_row) == 0) {  # Si les paramètres n'existent pas déjà
    # Ajouter les paramètres dans `param`
    new_param <- data.frame(a = a1, b = b1, c = c1, U0 = u0)
    param <- rbind(param, new_param)
    Nparam <-nrow(param)
    
    # Générer la suite avec bruit
    U_val_bruité <- Generate_output(N, a1, b1, c1, u0) 
    U_val_bruité[-1]<-U_val_bruité[-1]+ rnorm(N,mean=0,sd=1)
    # Ajouter les données dans Main_Learning_Base
    new_row <- data.frame(a = a1, b = b1, c = c1, U0 = u0, Un = I(list(U_val_bruité)), parameter_id = Nparam)
    Main_Learning_Base <- rbind(Main_Learning_Base, new_row)
    
# Générer la suite sans bruit
    U_val <- Generate_output(N, a1, b1, c1, u0)
    
    # Ajouter les données dans Output_Curves
    new_curve <- data.frame(a = a1, b = b1, c = c1, U0 = u0, Un = I(list(U_val)), parameter_id = Nparam)
    Output_Curves <- rbind(Output_Curves, new_curve)
  }
}

```



```{r}
RelDiff <- function(y_observe, y_pred) {
  return(rowMeans(abs(y_observe - y_pred)))
}
```






```{r}
f_obj<-function(A,B,C){
  N<-length(base_brutee)-4
  filtered_row <- subset(base_brutee, a == A & b == B & c == C)
  y_obser<-as.matrix(filtered_row[,4:(N+4)])
  y_pred<-t(mapply(Generate_output,N, A, B, C ,y_obser[,1]))
  ecart<-RelDiff(y_obser,y_pred)
  return(ecart)
  
}
```





```{r}
f_obj(1.72, 2.72, 0.80)
```

```{r}
N<-length(base_brutee)-4
filtered_row <- subset(base_brutee, a == 1.72 & b == 2.72 & c == 0.80)
y_obser<-as.matrix(filtered_row[,4:(N+4)])
y_pred<-t(mapply(Generate_output,N, 1.72, 2.72, 0.80 ,y_obser[,1]))
#ecart<-RelDiff(y_obser,y_pred)
```


```{r}
# Définir les bornes pour les paramètres a, b, et c selon les intervalles spécifiés
lower_bounds <- c(2, 1, 0.1)  # borne inférieure pour a, b et c
upper_bounds <- c(5, 5, 0.9)  # borne supérieure pour a, b et c

# Configurer et exécuter l’optimisation avec l'algorithme DIRECT
result <- nloptr(
  x0 = c(3.5, 3, 0.5),  # Valeurs de départ (peuvent être ajustées si besoin)
  eval_f = f_obj,       # Fonction d’objectif à minimiser
  lb = lower_bounds,    # Borne inférieure
  ub = upper_bounds,    # Borne supérieure
  opts = list("algorithm" = "NLOPT_GN_DIRECT", "maxeval" = 1000)  # Configuration DIRECT
)

# Afficher les résultats optimaux
print(result)

```







