---
title: "Machine Learning et ADC"
author: "Borice DOSSOU"
date: "2024-09-04"
output:
  html_document: default
  pdf_document: default
---

```{r}
rm(list = ls())
```

# Chargement des library
```{r}
library(ggplot2)
library(tidyr)
library(tidyverse)
library(reshape2)
```


Fonction generatrice de la suite
```{r}
# Définition de la fonction
Generate_output <- function(N, a, b, c, U0) {
  # Initialisation U0
  U <- numeric(N + 1)
  U[1] <- U0
  
  # Boucle pour calculer les valeurs de U_n
  for (n in 1:N) {
    U[n + 1] <- a * U[n]^c + b
  }
  
  return(U)
}
```


La fonction de test de function
_Essai 1_ 

```{r}
test_Generate_output <- function(){
 a <- 2
 b <- 2
 c <- 1
 U0 <- 1
 N <- 2
 waited<-c(2,2,2,1,1,4,10)
 fun_out<-c(N,a,b,c,Generate_output(N,a,b,c,U0))
 if(all(waited==fun_out)){
   return("OK")
 }else{
   return("Error...")
   }
}
test_Generate_output()
```

__Essai 2__

```{r}
test_Generate_output <- function(){
 a <- 2
 b <- 2
 c <- 1
 U0 <- 1
 N <- 1
 waited<-c(1,2,2,1,1,4)
 fun_out<-c(N,a,b,c,Generate_output(N,a,b,c,U0))
 if(all(waited==fun_out)){
   return("OK")
 }else{
   return("Error...")
   }
}
test_Generate_output()
```
Après avoir Exécuter le code de la fonction. Je constate que le code marche alors je peux le deployer.

Generer les courbes selons les valeurs des paramètres
Let N = 40, a ∈ [2; 5], b ∈ [1; 5] and , c ∈ [0.1; 0.9]

```{r}
# Pour rendre les résultats reproductibles (facultatif)
# Nombre d'itérations
N <- 40
U0 <- 2  # Valeur initiale de U(0)

# Paramètres pour la courbe 1
a1<-2
b1<-1
c1<-0.1
# Paramètres pour la courbe 2
a2<-5
b2<-5
c2<-0.9
# Paramètres pour la courbe 3
a3<-3.5
b3<-3
c3<-0.5
# Paramètres pour la courbe 4
a4 <- 4
b4 <- 2
c4 <- 0.3
# Paramètres pour la courbe 5
a5 <- 2.5
b5 <- 4
c5 <- 0.7
# Paramètres pour la courbe 6
a6 <- 3
b6 <- 1.5
c6 <- 0.4

# Pour permettre un affichage en grille des graphique
par(mfrow = c(3, 2))
# Générer et tracer la courbe 1
plot(0:N, Generate_output(N, a1,b1,c1, U0),
     type = "o", col = 1, ylim = c(0, 4), 
     xlab = "n", ylab = "U_n", 
     main = "Courbe de a=2, b=1, c=0.1")
# Générer et tracer la courbe 2
plot(0:N, Generate_output(N, a2,b2,c2, U0),
     type = "o", col = 1, ylim = c(0, 10^7), 
     xlab = "n", ylab = "U_n", 
     main = "Courbe de a=5, b=5, c=0.9")
# Générer et tracer la courbe 3
plot(0:N, Generate_output(N, a3,b3,c3, U0),
     type = "o", col = 1, ylim = c(0, 20), 
     xlab = "n", ylab = "U_n", 
     main = "Courbe de a=3.5, b=3, c=0.5")
# Générer et tracer la courbe 4
plot(0:N, Generate_output(N, a4, b4, c4, U0),
     type = "o", col = 1, ylim = c(0, 15), 
     xlab = "n", ylab = "U_n", 
     main = "Courbe de a=4, b=2, c=0.3")

# Générer et tracer la courbe 5
plot(0:N, Generate_output(N, a5, b5, c5, U0),
     type = "o", col = 1, ylim = c(0, 40), 
     xlab = "n", ylab = "U_n", 
     main = "Courbe de a=2.5, b=4, c=0.7")

# Générer et tracer la courbe 6
plot(0:N, Generate_output(N, a6, b6, c6, U0),
     type = "o", col = 1, ylim = c(0, 10), 
     xlab = "n", ylab = "U_n", 
     main = "Courbe de a=3, b=1.5, c=0.4")

```


Description des paramètres

a: contrôle le taux de croissance,
b: agit comme un décalage vertical,
c: modifie la non-linéarité de la croissance


__Épidémiologie__
Propagation des Maladies : Ce modèle peut également être utilisé pour modéliser la propagation des maladies infectieuses où le taux de propagation est influencé de manière non linéaire par le nombre d'individus infectés

__Dynamique des Populations__
Croissance des Populations : Ce modèle peut être utilisé pour modéliser la croissance des populations biologiques. 



__Exercise 2.2 Behavior of the parameters of the model__

1 ) La sensibilité des paramètres fait référence à la manière dont de petites variations des paramètres d'un modèle ou d'une équation affectent le comportement ou les résultats de ce modèle. En d'autres termes, si un paramètre change légèrement, la sensibilité mesure l'ampleur de la variation dans la sortie du modèle. Un système est dit sensible à un paramètre si une petite modification de ce dernier entraîne un changement significatif dans les résultats, tandis qu'il est insensible ou robuste si ces variations ont peu d'effet. La sensibilité est importante pour comprendre la stabilité d'un modèle et identifier les paramètres critiques qui influencent fortement le système."""

2) 


Approche : Fixer b et c à des valeurs constantes, puis augmenter ou diminuer progressivement a (par exemple, tester des valeurs dans l'intervalle [2,5].
Observer comment la vitesse de croissance de la suite change.
Sensibilité : Si de petites variations de a entraînent de grandes différences dans la croissance ou la divergence de la suite, cela indique une grande sensibilité au paramètrea.


```{r}
# Générer les valeurs des paramètres à partir des intervalles
a_values <- seq(from = 2, to = 5, length.out = 5)
b_values <- seq(from = 1, to = 5, length.out = 5)
c_values <- seq(from = 0.1, to = 0.9, length.out = 5)
U0_valeus<-seq(from = 1, to = 5, length.out = 5)
# Créer une grille de combinaisons possibles des paramètres
param_grid <- expand.grid(a = a_values, b = b_values, c = c_values, U0=U0_valeus)
param_grid
```



```{r}
calculate_gradient <- function(N, a, b, c, U0) {
  h_values <- seq(0.001, 0.05, length.out = 10) # Définir une plage de h
  fU <- Generate_output(N, a, b, c, U0)
  
  # Initialiser les vecteurs de variations
  total_variation_a <- 0
  total_variation_b <- 0
  total_variation_c <- 0
  
  for (h in h_values) {
    variation_a <- (Generate_output(N, a + h, b, c, U0) - fU) / h
    variation_b <- (Generate_output(N, a, b + h, c, U0) - fU) / h
    variation_c <- (Generate_output(N, a, b, c + h, U0) - fU) / h
    
    total_variation_a <- total_variation_a + variation_a
    total_variation_b <- total_variation_b + variation_b
    total_variation_c <- total_variation_c + variation_c
  }
  
  # Calcul de la moyenne des variations
  variat_a <- round(total_variation_a/ length(h_values) ,2)
  variat_b <- round(total_variation_b/ length(h_values) ,2)
  variat_c <- round(total_variation_c/ length(h_values) ,2)
  
  # Combiner les résultats en une matrice
  gradient_f <- cbind(variat_a, variat_b, variat_c, U0)
  
  return(gradient_f)
}
```



```{r}
calculate_gradient <- function(N, a, b, c, U0) {
  h_values <- seq(0.001, 0.05, length.out = 10) # Définir une plage de h
  fU <- Generate_output(N, a, b, c, U0)
  
  # Matrices pour stocker les variations pour chaque h
  variations_a <- matrix(0, nrow = length(fU), ncol = length(h_values))
  variations_b <- matrix(0, nrow = length(fU), ncol = length(h_values))
  variations_c <- matrix(0, nrow = length(fU), ncol = length(h_values))
  
  # Calcul des variations pour chaque h
  for (i in seq_along(h_values)) {
    h <- h_values[i]
    variations_a[, i] <- (Generate_output(N, a + h, b, c, U0) - fU) / h
    variations_b[, i] <- (Generate_output(N, a, b + h, c, U0) - fU) / h
    variations_c[, i] <- (Generate_output(N, a, b, c + h, U0) - fU) / h
  }
  
  # Calcul de la moyenne des variations pour chaque paramètre
  gradient_a <- rowMeans(variations_a)
  gradient_b <- rowMeans(variations_b)
  gradient_c <- rowMeans(variations_c)
  
  # Combiner les résultats en une matrice
  gradient_f <- round(cbind(gradient_a, gradient_b, gradient_c, U0),2)
  
  return(gradient_f)
}

```



```{r}
# Exemple d'utilisation
N <- 3
a <- 2
b <- 2
c <- 1
U0 <- 1

calculate_gradient(N, a, b, c, U0)

```






```{r}
test_calculate_gradient <- function() {
  # Paramètres pour le test
  N <- 3
  a <- 2
  b <- 2
  c <- 1
  U0 <- 1
  
  # Résultat attendu (valeurs connues ou calculées manuellement)
  waited <- c(
    0.00, 0.00, 0.00, 1,  # Ligne 1
    1.00, 1.00, 0.00, 1,  # Ligne 2
    6.03, 3.00, 11.29, 1, # Ligne 3
    22.20, 7.00, 72.05, 1 # Ligne 4
  )
  
  # Résultat obtenu
  gradients <- calculate_gradient(N, a, b, c, U0)
  fun_out <- as.vector(t(gradients)) # Transformer la matrice en vecteur ligne
  
  # Comparaison des résultats
  if (all.equal(waited, fun_out, tolerance = 1e-2)) {
    return("OK")
  } else {
    return("Error...")
  }
}

# Exécuter le test
test_calculate_gradient()

```














```{r}
# Calculer la norme du gradient
calculate_gradient_norm <- function(gradients) {
  # Calcul de la norme pour chaque ligne
  norms <- apply(gradients[,1:3], 1, function(row) sqrt(sum(row^2)))
  
  return(norms)
}
```



```{r}
calculate_sensitivity_indicateur <- function(gradients) {
  # Calculer la norme des gradients
  norms <- calculate_gradient_norm(gradients)
  
  # Indicateur relative pour chaque paramètre en utilisant max 

  Indcateur<-median(rowSums(gradients) /(norms+1))
  # Retourner les sensibilités
  return(Indcateur)
}
```



Stock des Indicateur
```{r}
Stock_sensitivity_params <- function(param_list, N) {
  # Initialiser une liste vide pour stocker les résultats
  stock <- data.frame(a = numeric(), b = numeric(), c = numeric(), U0 = numeric(), Indicator = numeric())
  
  # Parcourir chaque ensemble de paramètres dans la liste
  for (params in param_list) {
    a <- params$a
    b <- params$b
    c <- params$c
    U0 <- params$U0
    
    # Calculer les gradients pour cet ensemble de paramètres
    gradients <- calculate_gradient(N, a, b, c, U0)
    
    # Calculer l'indicateur de sensibilité pour les gradients obtenus
    indicator <- round(calculate_sensitivity_indicateur(gradients),2)
    
    # Ajouter les résultats à la base stock
    stock <- rbind(stock, data.frame(a = a, b = b, c = c, U0 = U0, Indicator = indicator))
  }
  
  return(stock)
}

```




```{r}
get_sensibility <- function(stock) {
  # Calculer la médiane de la colonne Indicator
  median_value <- median(stock$Indicator, na.rm = TRUE)
  
  # Trouver l'indice de la ligne dont la valeur d'Indicator est la plus proche de la médiane
  closest_index <- which.min(abs(stock$Indicator - median_value))
  
  # Retourner la ligne correspondante
  selected_row <- stock[closest_index, ]
  
  return(selected_row)
}

```


__les Fonctions test__


```{r}
test_calculate_gradient <- function() {
  N <- 2
  a <- 2
  b <- 2
  c <- 1
  U0 <- 1
  
  expected <- matrix(c(2.00, 0.00, 0.00, 1), nrow = 1) # Exemple de résultat attendu
  result <- calculate_gradient(N, a, b, c, U0)
  
  if (all.equal(result, expected)) {
    return("calculate_gradient: OK")
  } else {
    return("calculate_gradient: Error")
  }
}
test_calculate_gradient()

```
calculate_gradient <- function(N, a, b, c, U0)
```{r}
test_Generate_output <- function(){
 a <- 2
 b <- 2
 c <- 1
 U0 <- 1
 N <- 2
 waited<-c(2,2,2,1,1,4,10)
 fun_out<-c(N,a,b,c,Generate_output(N,a,b,c,U0))
 if(all(waited==fun_out)){
   return("OK")
 }else{
   return("Error...")
   }
}
test_Generate_output()
```


















```{r}
test_calculate_gradient_norm <- function() {
  gradients <- matrix(c(2.00, 0.00, 0.00, 1), nrow = 1) # Exemple d'entrée
  expected <- c(2.00) # Résultat attendu pour la norme
  
  result <- calculate_gradient_norm(gradients)
  
  if (all.equal(result, expected)) {
    return("calculate_gradient_norm: OK")
  } else {
    return("calculate_gradient_norm: Error")
  }
}

test_calculate_gradient_norm()

```


```{r}
test_calculate_sensitivity_indicateur <- function() {
  gradients <- matrix(c(2.00, 0.00, 0.00, 1), nrow = 1) # Exemple d'entrée
  expected <- 1.00 # Résultat attendu pour l'indicateur
  
  result <- calculate_sensitivity_indicateur(gradients)
  
  if (all.equal(result, expected)) {
    return("calculate_sensitivity_indicateur: OK")
  } else {
    return("calculate_sensitivity_indicateur: Error")
  }
}

test_calculate_sensitivity_indicateur()

```



```{r}
test_Stock_sensitivity_params <- function() {
  N <- 2
  param_list <- list(
    list(a = 2, b = 2, c = 1, U0 = 1),
    list(a = 3, b = 2, c = 1, U0 = 1)
  )
  
  expected <- data.frame(
    a = c(2, 3),
    b = c(2, 2),
    c = c(1, 1),
    U0 = c(1, 1),
    Indicator = c(1.00, 1.00) # Exemple de valeurs attendues
  )
  
  result <- Stock_sensitivity_params(param_list, N)
  
  if (all.equal(result, expected)) {
    return("Stock_sensitivity_params: OK")
  } else {
    return("Stock_sensitivity_params: Error")
  }
}

test_Stock_sensitivity_params()

```



```{r}
test_get_sensibility <- function() {
  stock <- data.frame(
    a = c(2, 3, 4),
    b = c(2, 3, 4),
    c = c(1, 1, 1),
    U0 = c(1, 1, 1),
    Indicator = c(1.00, 2.00, 3.00)
  )
  
  expected <- data.frame(
    a = 3,
    b = 3,
    c = 1,
    U0 = 1,
    Indicator = 2.00
  )
  
  result <- get_sensibility(stock)
  
  if (all.equal(result, expected)) {
    return("get_sensibility: OK")
  } else {
    return("get_sensibility: Error")
  }
}

test_get_sensibility()

```





























```{r}
# Exemple d'utilisation : définir plusieurs ensembles de paramètres
param_list <- list(
  list(a = 2, b = 2.5, c = 0.4, U0 = 1),
  list(a = 3.0, b = 4.0, c = 0.6, U0 = 1),
  list(a = 2.0, b = 3.0, c = 0.5, U0 = 1),
  list(a = 5.0, b = 5.0, c = 0.7, U0 = 1)
)

# Trouver les paramètres avec la meilleure sensibilité
Param_sim <- get_sensibility(Stock_sensitivity_params(param_list, N = 100))
print(Param_sim)

```

__Exercise 2.3 Distribution of the parameters__


Les paramètres d'entrée du modèle sont :

_a_ : Ce paramètre contrôle le facteur de croissance multiplicatif de la suite Un. Il influence directement la manière dont Un. Plus la valeur de a est élevée, plus la suite croît rapidement.

_b_ : Ce paramètre représente un terme constant qui est ajouté à chaque itération. Il détermine le biais ou la contribution fixe à chaque étape de la suite. Un b élevé entraîne.

_c_ : Ce paramètre contrôle la puissance à laquelle Un est élevé. Il modifie la manière non linéaire dont un influence sa propre évolution. Un c plus grand accentue les écarts, tandis qu'un c plus petit atténue la croissance de Un

_U0_  : La condition initiale, qui définit la valeur de départ de la suite U. C'est la première valeur qui déclenche l'évolution de la suite selon les paramètresa,b et c

_N_ : Le nombre d'itérations ou d'étapes, représentant la durée sur laquelle la suite est calculée.

_Sorties_ :
Les sorties du modèle sont : _Un_: La suite de valeurs obtenue après N itérations, où chaque valeur  Un dépend des paramètres a,b et c
ainsi que de la valeur précédente Un−1. C'est la sortie principale du modèle qui décrit l'évolution de la suite en fonction des entrées.



__Lois des paramètres a,b et c.__
Je simule une loi normale  pour mes paramètre
Mon param


```{r}
set.seed(123) # Pour des résultats reproductibles
n <- 50 # Nombre de simulations souhaité

# Simulation à partir des lois normales
sim_a <- rnorm(n, mean = 3, sd = 0.5)
sim_b <- rnorm(n, mean = 4, sd = 0.4)
sim_c <- rnorm(n, mean = 0.6, sd = 0.05)

sim_U0 <- round(runif(5, min = 1, max = 10),0)

# Créer les histogrammes
par(mfrow = c(1, 3))  # Pour afficher les 3 histogrammes côte à côte

hist(sim_a, main = "Histogramme de a", xlab = "a", col = "lightblue", breaks = 30)
hist(sim_b, main = "Histogramme de b", xlab = "b", col = "lightgreen", breaks = 30)
hist(sim_c, main = "Histogramme de c", xlab = "c", col = "lightcoral", breaks = 30)

```


__Exercise 3.1 The notion of noisy data__

__1. Notion de bruit dans la collecte de données__
Le bruit en collecte de données désigne les variations aléatoires ou les informations non pertinentes qui perturbent les données recueillies. Ces perturbations ne reflètent pas le phénomène réel mesuré. Le bruit peut provenir de sources variées, telles que des erreurs de mesure, des variations environnementales ou des biais dans les instruments de collecte.

__2. Problèmes causés par le bruit__
La présence de bruit peut entraîner plusieurs problèmes :

__Baisse de la précision__ : Le bruit rend les données moins fiables et rend difficile la distinction entre les variations aléatoires et les tendances réelles.
Difficulté à interpréter : Le bruit complique l'analyse, car il peut masquer les relations importantes entre les variables.
Modèles biaisés : En présence de bruit important, les modèles statistiques peuvent produire des résultats inexacts ou non représentatifs.

__3. Loi de probabilité du composant aléatoire__
Le composant aléatoire, c'est-à-dire le bruit, peut souvent être modélisé par une loi normale (gaussienne), car les erreurs et perturbations aléatoires ont tendance à suivre une distribution symétrique autour de zéro avec une variance spécifique. Cependant, d'autres distributions peuvent être utilisées selon la nature du bruit (loi uniforme pour des erreurs constantes, loi exponentielle pour des erreurs asymétriques, etc.). La loi normale est souvent privilégiée car elle reflète bien les erreurs aléatoires dans de nombreux contextes.

__Exercise 3.2 Generation of a learning database__


```{r}
# Simulation des paramètres pour n individus
set.seed(123)  # Pour la reproductibilité
M <- 100  # Nombre d'individus
N <- 40  # Nombre d'itérations


sim_a <- round(rnorm(M, mean = 2, sd = 0.5),2)  # Simulation des paramètres a
sim_b <- round(rnorm(M, mean = 3, sd = 0.4),2)  # Simulation des paramètres b
sim_c <- round(rnorm(M, mean = 0.6, sd = 0.09),2)  # Simulation des paramètres c
sim_Uo<-round(runif(M,min=1,max=10),2)
# Générer une base de données avec les valeurs de a, b, c et U[0], U[1], ..., U[N]
base_root <- data.frame(matrix(ncol = N + 4, nrow = M))  # Base vide (n lignes, N+4 colonnes)

# Noms des colonnes : a, b, c, U[0], ..., U[100]
colnames(base_root) <- c("a", "b", "c", paste0("U", 0:N))

# Remplir la base avec les valeurs simulées de a, b, c et les valeurs de U
for (i in 1:M) {
  base_root[i, 1] <- sim_a[i]  # Valeur de a
  base_root[i, 2] <- sim_b[i]  # Valeur de b
  base_root[i, 3] <- sim_c[i]  # Valeur de c
  base_root[i, 4] <- sim_Uo[i]  # Valeur de Uo
  U_values <- round(Generate_output(N, sim_a[i], sim_b[i], sim_c[i], sim_Uo[i]),2) #  
  base_root[i, 4:(N + 4)] <- U_values  # 
}

base_root %>%
  mutate(Individu = 1:M) %>%               # Ajouter la colonne 'Individu'
  melt(id.vars = c("a", "b", "c", "Individu"), 
       variable.name = "Iteration", 
       value.name = "U") %>%                # Restructurer le dataframe
  mutate(Iteration = as.numeric(gsub("U", "", Iteration))) %>%  # Convertir la colonne 'Iteration' en numérique
  ggplot(aes(x = Iteration, y = U, color = factor(Individu))) + # Graphique ggplot
  geom_line(size = 1) +                     # Tracer les courbes pour chaque individu
  labs(title = "Évolution de U[n] pour chaque individu",
       x = "Itération (N)", 
       y = "Valeur de U[N]",
       color = "Individu") +
  theme_minimal()    

```



```{r}
# Définition de la fonction avec ajout de bruit
Generate_output_with_noise <- function(N, a, b, c, U0) {
  U <- numeric(N + 1)
  U[1] <- U0
  
  for (n in 1:N) {
    # Calcul de U[n+1] avec ajout de bruit gaussien
    U[n + 1] <- a * U[n]^c + b + rnorm(1, mean = 0, sd = 1)
  }
  
  return(U)
}


# Générer une base de données avec les valeurs de a, b, c et U[0], U[1], ..., U[100]
base_brutee <- data.frame(matrix(ncol = N + 4, nrow = M))  # Base vide (n lignes, N+4 colonnes)

# Noms des colonnes : a, b, c, U[0], ..., U[100]
colnames(base_brutee) <- c("a", "b", "c", paste0("U", 0:N))

# Remplir la base avec les valeurs simulées de a, b, c et les valeurs de U
for (i in 1:M) {
  base_brutee[i, 1] <- sim_a[i]  # Valeur de a
  base_brutee[i, 2] <- sim_b[i]  # Valeur de b
  base_brutee[i, 3] <- sim_c[i]  # Valeur de c
  base_brutee[i, 4] <- sim_Uo[i]  # Valeur de Uo
  U_values_bruité <- Generate_output_with_noise(N, sim_a[i], sim_b[i], sim_c[i], sim_Uo[i])  # Générer U[0] à U[100] bruité  
  base_brutee[i, 4:(N + 4)] <- U_values_bruité  # Remplir les colonnes U[0] à U[100]
}

base_brutee %>%
  mutate(Individu = 1:M) %>%               # Ajouter la colonne 'Individu'
  melt(id.vars = c("a", "b", "c", "Individu"), 
       variable.name = "Iteration", 
       value.name = "U") %>%                # Restructurer le dataframe
  mutate(Iteration = as.numeric(gsub("U", "", Iteration))) %>%  # Convertir la colonne 'Iteration' en numérique
  ggplot(aes(x = Iteration, y = U, color = factor(Individu))) + # Graphique ggplot
  geom_line(size = 1) +                     # Tracer les courbes pour chaque individu
  labs(title = "Évolution de U[n] bruité pour chaque individu",
       x = "Itération (N)", 
       y = "Valeur de U[N]",
       color = "Individu") +
  theme_minimal()    

```




```{r}
#Main_Learning_Base <-round(rbind(base_brutee[, 4:(N + 4)],Output_Curves[, 4:(N + 4)]),2)
```


```{r}
set.seed(123)  # Pour la reproductibilité
Output_Curves<-base_root[, 5:(N + 4)]
matrice_bruit_aleatoire <- matrix(rnorm(M * N), nrow = M, ncol = N)
Main_Learning_Base<-Output_Curves+matrice_bruit_aleatoire
```

# Build SQL Base

# table des paramètres

```{r}
param<-base_root[, 1:4] %>% 
  mutate(ID=paste0("IND", 1:M)) %>% 
  select(ID,a,b,c,U0)
```

# Table donne avec bruit
```{r}
output_with_brute<-Main_Learning_Base %>%
  mutate(ID_p=paste0("IND", 1:M),ID_wb=paste0("OUTWB", 1:M)) %>%
  select(ID_wb,ID_p,everything())
```

# Table donne sans bruit
```{r}
output_brute<-Output_Curves %>%
  mutate(ID_p=paste0("IND", 1:M),ID_ob=paste0("OUTB", 1:M)) %>%
  select(ID_ob,ID_p,everything())
```



# Definir les base d'entrainement et de test

```{r}
# Supposons que Main_Learning_Base est déjà chargé et contient M lignes
set.seed(123)  # Pour la reproductibilité

# Déterminer le nombre de lignes pour chaque ensemble
train_size <- floor(0.7 * M)  # 70% pour l'entraînement
test_size <- M - train_size     # 30% pour le test

# Échantillonner les indices pour l'ensemble d'entraînement
train_indices <- sample(1:M, size = train_size, replace = FALSE)

# Créer les ensembles d'entraînement et de test
Main_Training_Base <- Main_Learning_Base[train_indices, ]  # Ensemble d'entraînement
Main_Test_Base <- Main_Learning_Base[-train_indices, ]      # Ensemble de test
```


Intérêt de cette approche :
L'approche vise à ajuster les paramètres d'un modèle (a,b,c) pour qu'il s'aligne au mieux avec des données réelles bruitées. Cela permet de trouver les paramètres qui minimisent la différence entre les prédictions du modèle et les observations. Ce processus est essentiel en modélisation pour :

Améliorer la précision des prédictions du modèle ;
Comprendre les relations sous-jacentes dans les données ;
Réduire l'erreur et ajuster le modèle aux particularités des données bruitées.



je veux ajouter d'autre ligne pour repondre à la fonction avec All_Uo


```{r}
# Nombre de termes à générer
N <- 40
U_0 <- c(7.18, 2.57, 3.87)

# Création d'une data frame vide avec N + 4 colonnes et autant de lignes que d'éléments dans U_0
nouvelle_ligne <- data.frame(matrix(ncol = N + 4, nrow = length(U_0)))

# Noms des colonnes : a, b, c, U[0], ..., U[N]
colnames(nouvelle_ligne) <- c("a", "b", "c", paste0("U", 0:N))

# Remplir les colonnes a, b, et c avec les mêmes valeurs
nouvelle_ligne$a <- 1.72
nouvelle_ligne$b <- 2.72
nouvelle_ligne$c <- 0.80

# Remplir chaque ligne avec une valeur différente de U_0 et générer la suite correspondante
for (i in seq_along(U_0)) {
  
  # Génération de la suite avec bruit pour chaque U_0[i]
  U_val_bruité <- Generate_output_with_noise(N, 1.72, 2.72, 0.80, U_0[i])
  
  # Remplir les colonnes U[1] à U[N] dans la ligne correspondante
  nouvelle_ligne[i, 4:(N + 4)] <- U_val_bruité
}
base_brutee <- rbind(base_brutee, nouvelle_ligne)
```




```{r}
RelDiff <- function(y_observe, y_pred) {
  return(rowMeans(abs(y_observe - y_pred)))
}
```






```{r}
f_obj<-function(A,B,C){
  N<-length(base_brutee)-4
  filtered_row <- subset(base_brutee, a == A & b == B & c == C)
  y_obser<-as.matrix(filtered_row[,4:(N+4)])
  y_pred<-t(mapply(Generate_output,N, A, B, C ,y_obser[,1]))
  ecart<-RelDiff(y_obser,y_pred)
  return(ecart)
  
}
```





```{r}
f_obj(1.72, 2.72, 0.80)
```

```{r}
N<-length(base_brutee)-4
filtered_row <- subset(base_brutee, a == 1.72 & b == 2.72 & c == 0.80)
y_obser<-as.matrix(filtered_row[,4:(N+4)])
y_pred<-t(mapply(Generate_output,N, 1.72, 2.72, 0.80 ,y_obser[,1]))
#ecart<-RelDiff(y_obser,y_pred)
```


```{r}
# Définir les bornes pour les paramètres a, b, et c selon les intervalles spécifiés
lower_bounds <- c(2, 1, 0.1)  # borne inférieure pour a, b et c
upper_bounds <- c(5, 5, 0.9)  # borne supérieure pour a, b et c

# Configurer et exécuter l’optimisation avec l'algorithme DIRECT
result <- nloptr(
  x0 = c(3.5, 3, 0.5),  # Valeurs de départ (peuvent être ajustées si besoin)
  eval_f = f_obj,       # Fonction d’objectif à minimiser
  lb = lower_bounds,    # Borne inférieure
  ub = upper_bounds,    # Borne supérieure
  opts = list("algorithm" = "NLOPT_GN_DIRECT", "maxeval" = 1000)  # Configuration DIRECT
)

# Afficher les résultats optimaux
print(result)

```







