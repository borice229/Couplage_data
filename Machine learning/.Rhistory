c = sim_c,
U0 = sim_U0
)
# Calcul des sorties Un et ajout comme une colonne
Main_Learning_Base$Un <- lapply(1:nrow(Main_Learning_Base), function(i) {
# Générer les valeurs de Un avec Generate_output
generated_values <- Generate_output(N, Main_Learning_Base$a[i], Main_Learning_Base$b[i], Main_Learning_Base$c[i], Main_Learning_Base$U0[i])
generated_values[-1] <- generated_values[-1] + rnorm(N, mean = 0, sd = 1)
return(generated_values)
})
# Établir la connexion
con <- dbConnect(RMySQL::MySQL(),
dbname = "Mlearn",  # Nom de la base de données
host = "127.0.0.1",    # Adresse locale du serveur MySQL
port = 3306,           # Port par défaut pour MySQL
user = "root",         # Nom d'utilisateur
password = "less@intsARS19") # Mot de passe réel
# Vérification de la connexion
if (!is.null(con)) {
cat("Connexion réussie à la base de données 'mlearning' !\n")
} else {
cat("Erreur lors de la connexion.\n")
}
param<-Main_Learning_Base[, 1:4]
# Créer la table Parameters
dbExecute(con, "
CREATE TABLE IF NOT EXISTS Parameters (
id INT AUTO_INCREMENT PRIMARY KEY,
a FLOAT NOT NULL,
b FLOAT NOT NULL,
c FLOAT NOT NULL,
U0 FLOAT NOT NULL
)
")
# Créer la table NormalData avec l'ID comme clé primaire et la clé étrangère pour parameter_id
dbExecute(con, "
CREATE TABLE IF NOT EXISTS NormalData (
id INT AUTO_INCREMENT PRIMARY KEY,  -- Définir id comme clé primaire
parameter_id INT NOT NULL,          -- Clé étrangère
Un JSON NOT NULL,                   -- Colonne JSON pour stocker Un
FOREIGN KEY (parameter_id) REFERENCES Parameters(id) ON DELETE CASCADE  -- Référence à Parameters
)
")
# Créer la table NoiseData
dbExecute(con, "
CREATE TABLE IF NOT EXISTS NoiseData (
id INT AUTO_INCREMENT PRIMARY KEY,
parameter_id INT NOT NULL,
Un JSON NOT NULL,
FOREIGN KEY (parameter_id) REFERENCES Parameters(id) ON DELETE CASCADE
)
")
# Désactiver les contraintes de clé étrangère
dbExecute(con, "SET foreign_key_checks = 0")
# Vider la table Parameters
dbExecute(con, "TRUNCATE TABLE Parameters")
# Réactiver les contraintes de clé étrangère
dbExecute(con, "SET foreign_key_checks = 1")
# Préparer les données à insérer, en excluant l'`id`
for (i in 1:nrow(param)) {
query <- sprintf("INSERT INTO Parameters (a, b, c, U0) VALUES (%f, %f, %f, %f)",
param$a[i], param$b[i], param$c[i], param$U0[i])
dbExecute(con, query)
}
# Créer la colonne 'parameter_id' qui contient des valeurs de 1 à N
Output_Curves$parameter_id <- 1:nrow(Output_Curves)
# Préparer les données à insérer dans la table NormalData
for (i in 1:nrow(Output_Curves)) {
# Convertir la colonne Un en chaîne JSON
Un_json <- toJSON(Output_Curves$Un[[i]], auto_unbox = TRUE)  # auto_unbox pour convertir en un tableau JSON correct
# Construire la requête SQL
query <- sprintf("INSERT INTO NormalData (parameter_id, Un) VALUES (%d, '%s')",
Output_Curves$parameter_id[i], Un_json)  # '%s' pour insérer la chaîne JSON
# Exécuter la requête d'insertion
dbExecute(con, query)
}
# Créer la colonne 'parameter_id' qui contient des valeurs de 1 à N
Main_Learning_Base$parameter_id <- 1:nrow(Main_Learning_Base)
# Préparer les données à insérer dans la table NormalData
for (i in 1:nrow(Main_Learning_Base)) {
# Convertir la colonne Un en chaîne JSON
Un_j <- toJSON(Main_Learning_Base$Un[[i]], auto_unbox = TRUE)  # auto_unbox pour convertir en un tableau JSON correct
# Construire la requête SQL
query <- sprintf("INSERT INTO NoiseData (parameter_id, Un) VALUES (%d, '%s')",
Main_Learning_Base$parameter_id[i], Un_j)  # '%s' pour insérer la chaîne JSON
# Exécuter la requête d'insertion
dbExecute(con, query)
}
# Supposons que Main_Learning_Base est déjà chargé et contient M lignes
set.seed(123)  # Pour la reproductibilité
# Déterminer le nombre de lignes pour chaque ensemble
train_size <- floor(0.7 * M)  # 70% pour l'entraînement
test_size <- M - train_size     # 30% pour le test
# Échantillonner les indices pour l'ensemble d'entraînement
train_indices <- sample(1:M, size = train_size, replace = FALSE)
# Créer les ensembles d'entraînement et de test
Main_Training_Base <- Main_Learning_Base[train_indices, ]  # Ensemble d'entraînement
Main_Test_Base <- Main_Learning_Base[-train_indices, ]      # Ensemble de test
Uo <- c(8.21, 1.572, 3.197)
a1<-1.72
b1<-2.72
c1<-0.80
# Vérifier et mettre à jour la base
for (u0 in Uo) {
# Vérifier si les paramètres (a, b, c, U0) existent déjà dans `param`
existing_row <- param[param$a == a1 & param$b == b1 & param$c == c1 & param$U0 == u0, ]
if (nrow(existing_row) == 0) {  # Si les paramètres n'existent pas déjà
# Ajouter les paramètres dans `param`
new_param <- data.frame(a = a1, b = b1, c = c1, U0 = u0)
param <- rbind(param, new_param)
Nparam <-nrow(param)
# Générer la suite avec bruit
U_val_bruité <- Generate_output(N, a1, b1, c1, u0)
U_val_bruité[-1]<-U_val_bruité[-1]+ rnorm(N,mean=0,sd=1)
# Ajouter les données dans Main_Learning_Base
new_row <- data.frame(a = a1, b = b1, c = c1, U0 = u0, Un = I(list(U_val_bruité)), parameter_id = Nparam)
Main_Learning_Base <- rbind(Main_Learning_Base, new_row)
# Générer la suite sans bruit
U_val <- Generate_output(N, a1, b1, c1, u0)
# Ajouter les données dans Output_Curves
new_curve <- data.frame(a = a1, b = b1, c = c1, U0 = u0, Un = I(list(U_val)), parameter_id = Nparam)
Output_Curves <- rbind(Output_Curves, new_curve)
}
}
RelDiff <- function(y_observe, y_pred) {
return(rowMeans(abs(y_observe - y_pred)))
}
test_RelDiff <- function() {
# Définir des valeurs d'observation et prédites
y_observe <- matrix(c(1, 2, 3, 4), nrow = 2, byrow = TRUE)
y_pred <- matrix(c(1, 2.5, 3, 4.5), nrow = 2, byrow = TRUE)
# Résultat attendu : moyenne des différences absolues
waited <- c(0.25, 0.25)
# Appel de la fonction RelDiff
fun_out <- RelDiff(y_observe, y_pred)
# Comparer les résultats attendus avec ceux obtenus
if(all(waited==fun_out)){
return("OK")
}else{
return("Error...")
}
}
# Appel de la fonction de test
test_RelDiff()
f_obj <- function(A, B, C) {
# Filtrer les lignes de la base principale
filtered_row <- subset(Main_Training_Base, a == A & b == B & c == C)
if (nrow(filtered_row) == 0) {
return(paste0('les paramètres renseignés ne sont pas dans la base')) # Retourner une valeur élevée si aucun filtre ne correspond
}
y_obser <- do.call(rbind, lapply(filtered_row$Un, function(x) unlist(x)))
# Déterminer N comme la longueur totale de `y_obser`
N<- ncol(y_obser)-1
# Générer les prédictions
y_pred <- t(mapply(Generate_output, N, A, B, C, filtered_row[, 4]))
# Calculer l'écart
ecart <- RelDiff(y_obser, y_pred)
return(ecart)
}
test_f_obj <- function() {
# Définir les valeurs des paramètres pour le test
A <- 3.09
B <- 4.43
C <- 0.70
# Résultat attendu pour ces paramètres
waited <- 0.8520035
# Appeler la fonction f_obj avec les paramètres donnés
fun_out <- f_obj(A, B, C)
# Comparer le résultat de la fonction avec le résultat attendu
if (all.equal(fun_out, waited, tolerance = 1e-6)) {
return("OK")
} else {
return("Error...")
}
}
# Appeler la fonction de test
test_f_obj()
f_obj(3.09,
4.43,
2)
#train parameters
params<-Main_Training_Base[, 1:3]
# Define the objective function to minimize
objectif <- function(p) {
A <- p[1]
B <- p[2]
C <- p[3]
n<-nrow(p)
fun <- 0
for (i in n ) {
fun <- fun + f_obj(A[i,], B[i,], C[i,])
}
fun
}
# Définir les bornes pour les paramètres a, b, et c
bornes_inf <- c(min(params$a), min(params$b), min(params$c))  # Bornes inférieures
bornes_sup <- c(max(params$a), max(params$b), max(params$c))  # Bornes supérieures
# Apply the DIRECT algorithm to optimize the parameters
resultat <- direct(fn = objectif, lower = bornes_inf, upper = bornes_sup, control = list(print.level = 1))
# Print the results (optimized parameters a, b, c)
print(resultat$par)  # Optimized values of a, b, and c
#set.seed(123)  # Pour garantir la reproductibilité du tirage
Sampl_train <- Main_Training_Base[sample(nrow(Main_Training_Base), 50, replace = TRUE), ]
param_spl<-Sampl_train[,1:3]
# Définir les bornes pour les paramètres a, b, et c
bornes_inf_spl <- c(min(param_spl$a), min(param_spl$b), min(param_spl$c))  # Bornes inférieures
bornes_sup_spl <- c(max(param_spl$a), max(param_spl$b), max(param_spl$c))  # Bornes supérieures
# Apply the DIRECT algorithm to optimize the parameters
resultat_spl <- direct(fn = objectif, lower = bornes_inf_spl, upper = bornes_sup_spl, control = list(print.level = 1))
# Print the results (optimized parameters a, b, c)
print(resultat_spl$par)  # Optimized values of a, b, and c
# Créer un tableau vide pour stocker les résultats des 10 itérations
results_table <- data.frame(a = numeric(10), b = numeric(10), c = numeric(10))
# Répéter l'opération 10 fois
for (i in 1:10) {
# Tirage aléatoire avec remise de 50 courbes
indices <- sample(nrow(Main_Training_Base), 50, replace = TRUE)
Newbase_spl <- Main_Training_Base[indices, ]
param_sp<-Newbase_spl[,1:3]
# Définir les bornes pour les paramètres a, b, et c
bornes_inf_sp <- c(min(param_sp$a), min(param_sp$b), min(param_sp$c))  # Bornes inférieures
bornes_sup_sp <- c(max(param_sp$a), max(param_sp$b), max(param_sp$c))  # Bornes   supérieures
# Appliquer l'algorithme DIRECT pour ajuster les paramètres
resultat_sp <- direct(fn = objectif, lower = bornes_inf_sp, upper = bornes_sup_sp, control = list(print.level = 0))
# Stocker les résultats des paramètres optimisés dans le tableau
results_table[i, ] <- round(resultat_sp$par,2)
}
# Afficher le tableau contenant les valeurs des 10 triplets
print(results_table)
# Calcul de la moyenne, de la variance et de l'écart-type relatifs
moyenne <- colMeans(results_table)
variance <- apply(results_table, 2, var)
ecart_type <- sqrt(variance)
# Calcul de la variance relative et de l'écart-type relatif
# Variance relative = variance / (moyenne^2)
variance_relative <- variance / (moyenne^2)
# Ecart-type relatif = écart-type / moyenne
ecart_type_relatif <- ecart_type / moyenne
# Afficher les résultats
resultats_statistiques <- data.frame(
Moyenne = round(moyenne,2),
Variance_Relative = variance_relative,
Ecart_Type_Relatif = round(ecart_type_relatif,2)
)
print(resultats_statistiques)
a_dj<-3
b_dj<-4.18
c_dj<-0.62
out_size<-40
# Fonction pour calculer le MAE pour chaque ligne
calculate_mae_per_row <- function(y_real_data, y_pred_data) {
# Vérifier que les dimensions des matrices correspondent
if (nrow(y_real_data) != nrow(y_pred_data)) {
stop("Les matrices y_real_data et y_pred_data doivent avoir le même nombre de lignes.")
}
# Calcul du MAE pour chaque ligne
mae_per_row <- apply(cbind(y_real_data, y_pred_data), 1, function(row) {
# Extraire la ligne de valeurs réelles et prédites
y_real <- row[1:ncol(y_real_data)]
y_pred <- row[(ncol(y_real_data) + 1):length(row)]
# Calcul de l'erreur absolue pour la ligne
mae <- mean(abs(y_real - y_pred))
return(mae)
})
# Calcul de la moyenne des MAE
mae_global <- mean(mae_per_row)
return(mae_global)
}
# Exemple d'utilisation
# Matrices de valeurs réelles et prédites (par exemple 3 observations avec 2 variables)
y_real_data <- do.call(rbind, lapply(Main_Training_Base$Un, function(x) unlist(x)))
y_pred_data <- t(mapply( Generate_output ,out_size,a_dj, b_dj ,c_dj, Main_Training_Base$U0))
# Calcul du MAE global
mae_train <- calculate_mae_per_row(y_real_data, y_pred_data)
# Affichage du résultat
print(paste("Le MAE train est :", mae_train))
a_dj<-3
b_dj<-4.18
c_dj<-0.62
out_size<-40
# Exemple d'utilisation
# Matrices de valeurs réelles et prédites (par exemple 3 observations avec 2 variables)
y_real_test <- do.call(rbind, lapply(Main_Test_Base$Un, function(x) unlist(x)))
y_pred_test <- t(mapply( Generate_output ,out_size,a_dj, b_dj ,c_dj, Main_Test_Base$U0))
# Calcul du MAE global
mae_test <- calculate_mae_per_row(y_real_test, y_pred_test)
# Affichage du résultat
print(paste("Le MAE test est :", mae_test))
MAE_global <- function(y_observe, y_pred_list) {
mae_per_u0 <- sapply(seq_along(y_pred_list), function(i) {
mean(abs(y_observe[[i]] - y_pred_list[[i]]))
})
return(mean(mae_per_u0))
}
gradient_descent <- function(x, y, m, b, alpha, iterations) {
n <- length(y)  # Number of data points
cost_history <- numeric(iterations)  # To store the cost at each iteration
for (i in 1:iterations) {
# Predicted values
y_pred <- m * x + b
# Calculate gradients
gradient_m <- -(2/n) * sum(x * (y - y_pred))  # Gradient for slope (m)
gradient_b <- -(2/n) * sum(y - y_pred)  # Gradient for intercept (b)
# Update parameters
m <- m - alpha * gradient_m
b <- b - alpha * gradient_b
# Calculate and store the cost (Mean Squared Error)
cost <- sum((y - y_pred)^2) / n
cost_history[i] <- cost
# Print the cost every 100 iterations
if (i %% 100 == 0) {
cat("Iteration:", i, " Cost:", cost, "\n")
}
}
return(list(m = m, b = b, cost_history = cost_history))
}
# Run the gradient descent algorithm
result <- gradient_descent(x, y, m, b, alpha, iterations)
Ntrain
Ntrain<-0.3*100*41
# Construire Train_Base_1 avec les 30% premières lignes
Train_Base_1 <- Learn_data[1:Ntrain, ]
ML_data<-head(Main_Learning_Base,-3)
Learn_data <- ML_data%>%
# Décomposer la liste Un en plusieurs lignes
unnest(Un) %>%
# Ajouter une nouvelle colonne avec le numéro de courbe
mutate(Ncurve = rep(1:nrow(ML_data), times = sapply(ML_data$Un, length))) %>%
# Ajouter l'indice n pour chaque élément dans Un
group_by(Ncurve) %>%
mutate(n = row_number()) %>%
ungroup() %>%
select(Ncurve, n, U0, TS = Un)
Ntrain<-0.3*100*41
# Construire Train_Base_1 avec les 30% premières lignes
Train_Base_1 <- Learn_data[1:Ntrain, ]
Test_Base_1<-Learn_data[(Ntrain + 1):nrow(Learn_data) , ]
# Standardisation des données
Train_Base_1_scaled <- Train_Base_1
Train_Base_1_scaled[, c("n", "U0", "TS")] <- scale(Train_Base_1[, c("n", "U0", "TS")])
# Entraîner le modèle de réseau de neurones sur les données standardisées
nn_model <- nnet(TS ~ n + U0,
data = Train_Base_1_scaled,
size = c(3, 6, 3),
linout = TRUE,
trace = FALSE)
# Vérifier si des valeurs infinies ou extrêmes existent dans les données
any(is.infinite(Train_Base_1$n))  # Vérifier dans la colonne n
any(is.infinite(Train_Base_1$U0)) # Vérifier dans la colonne U0
any(is.infinite(Train_Base_1$TS)) # Vérifier dans la colonne TS
# Afficher les lignes qui contiennent des valeurs infinies
Train_Base_1[is.infinite(Train_Base_1$n), ]
Train_Base_1[is.infinite(Train_Base_1$U0), ]
Train_Base_1[is.infinite(Train_Base_1$TS), ]
?nnet
# Standardisation des données
Train_Base_1_scaled <- Train_Base_1
Train_Base_1_scaled[, c("n", "U0", "TS")] <- scale(Train_Base_1[, c("n", "U0", "TS")])
# Entraîner le modèle de réseau de neurones sur les données standardisées
nn_model <- nnet(TS ~ n + U0,
data = Train_Base_1_scaled,
size = c(3, 6, 3),
linout = TRUE,
trace = FALSE)
View(Train_Base_1_scaled)
# Standardisation des données
Train_Base_1_scaled <- Train_Base_1
Train_Base_1_scaled[, c("n", "U0", "TS")] <- scale(Train_Base_1[, c("n", "U0", "TS")])
# Entraîner le modèle de réseau de neurones sur les données standardisées
nn_model <- nnet(TS ~ n + U0,
data = Train_Base_1_scaled,
size = c(3, 6, 3),
linout = TRUE,
trace = FALSE)
apply(Train_Base_1_scaled[, c("n", "U0", "TS")], 2, var)
# Vérifier si des valeurs manquantes existent
colSums(is.na(Train_Base_1_scaled))
# Vérifier si des valeurs infinies existent
any(is.infinite(Train_Base_1_scaled$n))  # Vérifier dans la colonne n
any(is.infinite(Train_Base_1_scaled$U0)) # Vérifier dans la colonne U0
any(is.infinite(Train_Base_1_scaled$TS)) # Vérifier dans la colonne TS
# Entraîner le modèle avec plus d'itérations
nn_model <- nnet(TS ~ n + U0,
data = Train_Base_1_scaled,
size = c(3, 6, 3),
linout = TRUE,
trace = FALSE,
maxit = 1000)  # Augmenter le nombre d'itérations
# Exemple d'entraînement avec keras (réseau de neurones à 3 couches cachées)
library(keras)
install.packages("keras")
# Exemple d'entraînement avec keras (réseau de neurones à 3 couches cachées)
library(keras)
# Définir le modèle
model <- keras_model_sequential() %>%
layer_dense(units = 3, activation = "relu", input_shape = c(2)) %>%
layer_dense(units = 6, activation = "relu") %>%
layer_dense(units = 3, activation = "linear")
sum(is.na(Train_Base_1_scaled))
Train_Base_1_scaled
# Exemple d'entraînement avec keras (réseau de neurones à 3 couches cachées)
library(keras)
# Définir le modèle Keras
model <- keras_model_sequential() %>%
layer_dense(units = 3, activation = "relu", input_shape = c(2)) %>%
layer_dense(units = 6, activation = "relu") %>%
layer_dense(units = 3, activation = "linear")
library(neuralnet)
install.packages("neuralnet")
library(neuralnet)
set.seed(333)
nc <- neuralnet(TS~n +U0,
data = Train_Base_1_scaled,
hidden = 5,
err.fct = "ce",
linear.output = FALSE,
lifesign = 'full',
rep = 2,
algorithm = "rprop+",
stepmax = 100000)
?neuralnet
library(neuralnet)
set.seed(333)
nc <- neuralnet(TS~n +U0,
data = Train_Base_1_scaled,
hidden = 5,
err.fct = "sse",
linear.output = FALSE,
lifesign = 'full',
rep = 2,
algorithm = "rprop+",
stepmax = 100000)
library(neuralnet)
set.seed(333)
nc <- neuralnet(TS~n +U0,
data = Train_Base_1_scaled,
hidden = 5,
err.fct = "sse",
linear.output = FALSE,
lifesign = 'full',
rep = 2,
algorithm = "rprop+",
stepmax = 100)
library(neuralnet)
# Assurez-vous que les données sont standardisées (vous l'avez déjà fait avec scale)
set.seed(333)
# Ajuster le réseau de neurones avec 3 couches cachées et 3, 6, 3 neurones respectivement
nc <- neuralnet(TS ~ n + U0,
data = Train_Base_1_scaled,  # Train_Base_1 standardisé
hidden = c(3, 6, 3),         # 3 couches cachées avec 3, 6, et 3 neurones
err.fct = "sse",             # Fonction d'erreur : Sum of Squared Errors
linear.output = FALSE,       # Sortie non linéaire (si nécessaire pour la régression)
lifesign = 'full',           # Affiche des informations détaillées pendant l'entraînement
rep = 2,                     # Nombre de répétitions (itérations) du modèle
algorithm = "rprop+",        # Algorithme utilisé : "rprop+" (Resilient Backpropagation)
stepmax = 100)               # Nombre maximal de pas (itérations)
# Résumé du modèle entraîné
summary(nc)
library(neuralnet)
# Assurez-vous que les données sont standardisées (vous l'avez déjà fait avec scale)
set.seed(333)
nc <- neuralnet(TS ~ n + U0,
data = Train_Base_1_scaled,
hidden = c(3, 6, 3),
err.fct = "sse",
linear.output = FALSE,
lifesign = 'full',
rep = 2,
algorithm = "rprop+",
stepmax = 500,
thresh = 0.001)  # Réduire le seuil de convergence
# Résumé du modèle entraîné
summary(nc)
library(neuralnet)
# Assurez-vous que les données sont standardisées (vous l'avez déjà fait avec scale)
set.seed(333)
nc <- neuralnet(TS ~ n + U0,
data = Train_Base_1_scaled,
hidden = c(3, 6, 3),
err.fct = "sse",
linear.output = FALSE,
lifesign = 'full',
rep = 10,
algorithm = "rprop+",
stepmax = 500,
thresh = 0.001)  # Réduire le seuil de convergence
# Résumé du modèle entraîné
summary(nc)
Entrée (n, U0)
library(neuralnet)
# Assurez-vous que les données sont standardisées (vous l'avez déjà fait avec scale)
set.seed(333)
Train_Base_1_scaled<-scale(Train_Base_1)
nc <- neuralnet(TS ~ n + U0,
data = Train_Base_1_scaled,
hidden = c(3, 6, 3),
err.fct = "sse",
linear.output = FALSE,
lifesign = 'full',
rep = 10,
algorithm = "rprop+",
stepmax = 700,
thresh = 0.001)  # Réduire le seuil de convergence
# Résumé du modèle entraîné
summary(nc)
Train_Base_1_S <- Train_Base_1
Train_Base_1_S[, c("n", "U0", "TS")] <- scale(Train_Base_1_S[, c("n", "U0", "TS")])
Test_Base_1_S <- Test_Base_1
Test_Base_1_S[, c("n", "U0", "TS")] <- scale(Test_Base_1_S[, c("n", "U0", "TS")])
library(ggplot2)
t
Train_Base_1_S$Predicted_TS <- predict(nc, newdata = Train_Base_1_S)
