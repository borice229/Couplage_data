Y[support]
X[support,]
alpha[support]*Y[support]*X[support,]
colSums(alpha[support]*Y[support]*X[support,])
#C'est le w etoile
w<-colSums(alpha[support]*Y[support]*X[support,])
# Calcul du biais (b) en utilisant un vecteur support
i <- support[1]  # On peut choisir le premier vecteur de support
b <- Y[i] - sum(w * X[i,])  # b = y_i - <w, x_i> pour un vecteur support
# Fonction pour calculer x2 en fonction de x1 pour l'hyperplan
f <- function(x1, w, b) {
(-w[1] * x1 - b) / w[2]
}
# Tracer les points avec les deux classes
plot(X, typ = "n", xlab = "X1", ylab = "X2", main = "Hyperplan avec Vecteurs de Support")
points(X[Y == 1,], col = "blue", pch = 24)
points(X[Y == -1,], col = "red", pch = 24)
# Tracer l'hyperplan
abline(a = -b / w[2], b = -w[1] / w[2], col = "black", lwd = 2)
# Mettre en évidence les vecteurs de support
points(X[support,], col = "purple", pch = 19)  # Vecteurs de support en violet
# Tracer le vecteur wo de trois differentes facon
# Calcule de Wo
w01<-1/Y[support[1]]-t(w)%*%X[support[1],]
w02<-1/Y[support[2]]-t(w)%*%X[support[2],]
w03<-1/Y[support[3]]-t(w)%*%X[support[3],]
H_x <- function(X, Y, alpha, newdata, support) {
# Calculer w0 pour un vecteur de support
k <- support[1]
w0 <- Y[k] - sum(alpha[support] * Y[support] * (X[support, ] %*% X[k, ]))
# Classer plusieurs points
result <- colSums(alpha[support] * Y[support] * (X[support, ] %*% t(newdata))) + w0
return(result)
}
w0 <- Y[k] - sum(alpha[support] * Y[support] * (X[support, ] %*% X[k, ]))
# Classer plusieurs points
result <- colSums(alpha[support] * Y[support] * (X[support, ] %*% t(newdata))) + w0
H_x <- function(X, Y, alpha, newdata, support) {
# Calculer w0 pour un vecteur de support
k <- support[1]
w0 <- Y[k] - sum(alpha[support] * Y[support] * (X[support, ] %*% X[k, ]))
# Classer plusieurs points
result <- colSums(alpha[support] * Y[support] * (X[support, ] %*% t(newdata))) + w0
return(result)
}
# Définir les nouveaux points à classifier
x1 <- c(6, 0)
x2 <- c(6, 4)
x3 <- c(4, 4)
x4 <- c(4, 2)
x5 <- c(2, 3)
x6 <- c(0, 4.5)
X_new <- rbind(x1, x2, x3, x4, x5, x6)
# Classer les points avec la fonction H_x
results <- H_x(X, Y, alpha, newdata = X_new, support = support)
results
# Définir une grille de points
XXnew <- expand.grid(seq(-2, 8, length = 40), seq(-2, 8, length = 40))
# Classer les points de la grille
grid_results <- H_x(X, Y, alpha, newdata = as.matrix(XXnew), support = support)
# Convertir les résultats en un format adapté pour ggplot2
grid_results_matrix <- matrix(grid_results, nrow = 40, ncol = 40)
grid_results_df <- as.data.frame(XXnew)
grid_results_df$classification <- as.factor(ifelse(grid_results > 0, "Class 1", "Class -1"))
ggplot(grid_results_df, aes(x = Var1, y = Var2, color = classification)) +
geom_point(size = 0.5) +
scale_color_manual(values = c("blue", "red")) +
labs(title = "Classification Grid", x = "X1", y = "X2") +
theme_minimal()
# Entraînement SVM linéairement séparable
svm_model <- svm(X,Y, kernel = "linear", cost = 10, scale = FALSE)
library(kernlab)
# Entraînement SVM linéairement séparable
svm_model <- svm(X,Y, kernel = "linear", cost = 10, scale = FALSE)
?svm
# Entraînement SVM linéairement séparable
svm_model <- SVM(X,Y, kernel = "linear", cost = 10, scale = FALSE)
# Entraînement SVM linéairement séparable
svm_model <- svm(X,Y, kernel = "linear", cost = 10, scale = FALSE)
library(e1071)
# Entraînement SVM linéairement séparable
svm_model <- svm(X,Y, kernel = "linear", cost = 10, scale = FALSE)
# Affichage des résultats
plot(svm_model, X, col = c("blue", "red"))
w <- t(svm_model$coefs) %*% svm_model$SV
b <- -svm_model$rho
cat("w:", w, "\n")
cat("b:", b, "\n")
# Affichage des résultats
plot(svm_model, X, col = c("blue", "red"))
# Affichage des résultats
curve(svm_model, X, col = c("blue", "red"))
# Affichage des résultats
plot(svm_model, X, col = c("blue", "red"))
svm_model
svm_model$coefs
svm_model$SV
# Affichage des résultats
plot(svm_model, X, col = c("blue", "red"))
w <- t(svm_model$coefs) %*% svm_model$SV
b <- -svm_model$rho
cat("w:", w, "\n")
cat("b:", b, "\n")
return (sin(2*pi*x^3))^3
(sin(2*pi*x^3))^3
return (sin(2*pi*x^3))^3}
m_x <- function(x){
return (sin(2*pi*x^3))^3
}
xi<-runif(n)
# Simulation
n=1000
si<- rnorm(n,mean=0, sd=sqrt(0.1))
yi<-m_x(xi) + si
rm(list)
yi<-m_x(xi) + si
# Simulation
n=1000
xi<-runif(n)
si<- rnorm(n,mean=0, sd=sqrt(0.1))
m_x <- function(x){
return (sin(2*pi*x^3))^3
}
yi<-m_x(xi) + si
plot(xi,yi)
seed=123
library(tidyverse)
rectangle <- function(x)
{
0.5*(abs(x)<=1)
}
xi<-sort(runif(n))
yi<-m_x(xi) + si
plot(xi,yi)
xi<-sort(runif(n))
yi<-m_x(xi) + si
install.packages("parallel")
install.packages("parallel")
install.packages("parallel")
library(parallel, lib.loc = "C:/Users/boric/AppData/Local/R/cache/R/renv/sandbox/windows/R-4.4/x86_64-w64-mingw32/88765555")
library(parallel, lib.loc = "C:/Users/boric/AppData/Local/R/cache/R/renv/sandbox/windows/R-4.4/x86_64-w64-mingw32/88765555")
install.packages("parallel")
install.packages("parallel")
library(parallel)
library(doParallel)
no_cores <- detectCores() - 1
cl <- makeCluster(no_cores)
cl
no_cores
# Simulation
n<-1000
sigma<-sqrt(0.1)
xi<-sort(runif(n))
si<- rnorm(n,mean=0, sd=sigma)
m_x <- function(x){
return (sin(2*pi*x^3))^3
}
yi<-m_x(xi) + si
plot(xi,yi)
NW <- function(x,Xi,Yi,h,noy="G")
{
if (noy=="G")
noyau=gauss
if (noy=="R")
noyau=rectangle
if (noy=="T")
noyau=triangle
if (noy=="E")
noyau=epanechnikov
dXh <- (x-Xi)/h
num <- sum( noyau(dXh)*Yi )
denom <- sum( noyau(dXh) )
num/denom
return(num/denom)
}
data <- cbind(Xi,yi)
data <- cbind(xi,yi)
data
data <- data.frame(cbind(xi,yi))
View(data)
data <- data.frame(cbind(xi,yi,m_x,si))
View(data)
data <- data.frame(cbind(xi,yi,m_x(xi),si))
data <- data.frame(cbind(xi,yi,mx=m_x(xi),si))
Nada_wa <- function(x,Xi,Yi,h,noy="G")
{
if (noy=="G")
noyau=gauss
if (noy=="R")
noyau=rectangle
if (noy=="T")
noyau=triangle
if (noy=="E")
noyau=epanechnikov
dXh <- (x-Xi)/h
num <- sum( noyau(dXh)*Yi )
denom <- sum( noyau(dXh) )
num/denom
return(num/denom)
}
a <- list()
a[1]<-2
a
a[2]<-5
a
a
f<-c(1,2,3)
g<-c(1,6,3)
s<-cbind(f,g)
s[1]
s
s[,1]
s[1,1]
s<-data.frame(s)
s
s[1,1]
s[,2]
CV <- function(x,Xi,Yi,h,noy="G"){
Nada_Wat<-data(cbind(h,mapply(Nada_wa, x,Xi,Yi,h,noy="G"))
N<-length(h)
CV <- function(x,Xi,Yi,h,noy="G"){
Nada_Wat<-data(cbind(h,mapply(Nada_wa, x,Xi,Yi,h,noy="G")))
N<-length(h)
cv<-rep(0,N)
for (i in N ){
cv <-sum((Yi -Nada_Wat[i,2])^2)
}
return (stock<-which.min(cbind(h,cv)))
}
plot(data$xi,data$yi)
curve(vraifonction,add=T,col="red")
curve(Nada_wa,add=T,col="red")
# Noyaux utilisés
rectangle <- function(x)
{
0.5*(abs(x)<=1)
}
triangle <- function(x)
{
(1-abs(x))*(abs(x)<=1)
}
gauss <- function(x)
{
(1/(2*pi)^0.5)*exp(-x^2/2)
}
epanechnikov <- function(x)
{
((3/4)*(1-x^2))*(abs(x)<=1)
}
plot(data$xi,data$yi)
curve(Nada_wa,add=T,col="red")
# Calcul pour différents h (h=0.5, h=0.1 et h=0.01)
h <- 0.5
resu1 <- NULL
for(i in 1: n)
{
print(i)
resu1[i] = Nada_wa(data$xi[i],data$xi,data$yi,h)
}
lines(data$xi,resu1,col="blue",lty=1)
h <- 0.1
resu1 <- NULL
for(i in 1: n)
{
print(i)
resu1[i] = Nada_wa(data$xi[i],data$xi,data$yi,h)
}
lines(data$xi,resu1,col="red",lty=1)
h <- 0.09
resu1 <- NULL
for(i in 1: n)
{
resu1[i] = Nada_wa(data$xi[i],data$xi,data$yi,h)
}
lines(data$xi,resu1,col="yellow",lty=1
# Calcul pour différents h (h=0.5, h=0.1 et h=0.01)
h <- 0.09
h <- 0.09
resu1 <- NULL
for(i in 1: n)
{
resu1[i] = Nada_wa(data$xi[i],data$xi,data$yi,h)
}
lines(data$xi,resu1,col="yellow",lty=1)
h <- 0.05
plot(data$xi,data$yi)
curve(Nada_wa,add=T,col="red")
h<-rnorm(10,mean=0,sd=0.01)
CV(data$xi[1],data$xi,data$yi,h)
h
mapply(Nada_wa,data$xi[1],data$xi,data$yi,h)
new_data<- read.csv("Dat\DBH", header= T)
new_data<- read.csv("Dat/DBH", header= T)
new_data<- read.csv("\Dat\DBH", header= T)
new_data<- read.csv(".\Dat\DBH", header= T)
new_data<- read.csv("\Dat\DBH.txt", header= T)
new_data<- read.csv("DBH.txt", header= T)
new_data<- read.csv("DBH.txt", header= T)
getwd()
setwd("C:/Users/boric/OneDrive/Documents/Machine learning")
getwd()
new_data<- read.csv("DBH.txt", header= T)
new_data<- read.csv("\DBH.txt", header= T)
new_data<- read.table("/DBH.txt", header= F)
new_data<- read.table("DBH.txt", header= F)
new_data<- read.table("DBH.txt")
new_data<- read.table("DBH.txt")
View(new_data)
new_data_V6<- read.table("DBH.txt") %>%
filter(V1=6)
new_data_V6<- read.table("DBH.txt") %>%
filter(V1=6)
new_data_V6<- read.table("DBH.txt") %>%
filter(V1==6)
View(new_data_V6)
plot(new_data_V6$V2)
plot(new_data_V6$V3)
# V1 ce sont les indivus ici les hurtres
# V2 le temps
# V3 l'ecart en mm
new_data_V6<- read.table("DBH.txt") %>%
filter(V1==6) %>%
rename(ind=V1,temp=V2,Ecart=V3)
plot(new_data_V6$Ecart)
library(doParallel)
library(tidyverse)
no_cores <- detectCores() - 1
cl <- makeCluster(no_cores)
regressiondata <- function(n)
{
X <- sort(runif(n))
m <- (sin(2*pi*X^3))^3
e <- rnorm(n,0,sqrt(0.02))
y <- m+e
result <- list(x=X,y=y,m=m,e=e)
}
vraifonction <- function(x)
{
res <- (sin(2*pi*x^3))^3
return(res)
}
n <- 1000
data <- regressiondata(n)
plot(data$x,data$y)
curve(vraifonction,add=T,col="red")
# Noyaux utilisés
rectangle <- function(x)
{
0.5*(abs(x)<=1)
}
triangle <- function(x)
{
(1-abs(x))*(abs(x)<=1)
}
gauss <- function(x)
{
(1/(2*pi)^0.5)*exp(-x^2/2)
}
epanechnikov <- function(x)
{
((3/4)*(1-x^2))*(abs(x)<=1)
}
# Estimateur de Nadaraya-Watson
NW <- function(x,X,Y,h,noy="G")
{
if (noy=="G")
noyau=gauss
if (noy=="R")
noyau=rectangle
if (noy=="T")
noyau=triangle
if (noy=="E")
noyau=epanechnikov
dXh <- (x-X)/h
num <- sum( noyau(dXh)*Y )
denom <- sum( noyau(dXh) )
num/denom
return(num/denom)
}
plot(data$x,data$y)
curve(vraifonction,add=T,col="red")
# Calcul pour différents h (h=0.5, h=0.1 et h=0.01)
h <- 0.5
resu1 <- NULL
for(i in 1: n)
or(i in 1: n)
for(i in 1: n)
{
print(i)
resu1[i] = NW(data$x[i],data$x,data$y,h)
}
lines(data$x,resu1,col="blue",lty=1)
h <- 0.1
resu2 <- NULL
for(i in 1: n)
{
print(i)
resu2[i] = NW(data$x[i],data$x,data$y,h)
}
lines(data$x,resu2,col="green",lty=2)
h <- 0.01
resu3 <- NULL
for(i in 1: n)
{
print(i)
resu3[i] = NW(data$x[i],data$x,data$y,h)
}
lines(data$x,resu3,col="pink")
Mat <- cbind(data$x,data$y,resu1,resu2,resu3)
View(Mat)
CV <- function(X,Y,h,noy="G")
{
E <- NULL
for (i in 1:n)
{
E[i] <- (Y[i]- NW(X[i],X[-i],Y[-i],h,noy))^2
}
return(mean(E))
}
hCV <- function(X,Y,noy="G")
{
CV1 <- function(h)
{
CV(X,Y,h,noy="G")
#print(CV(X,Y,h,noy="G"))
}
#grilleh=1:200/1000
#E <- sapply(grilleh,CV1)
E <- mclapply(grilleh,CV1,mc.cores=no_cores)
return(list(h=grilleh[which.min(E)],erreur=E))
}
n <- length(data$x)
grilleh <- 1:200/1000
T1<-Sys.time()
hopt <- hCV(data$x,data$y)
no_cores <- detectCores() - 1
cl <- makeCluster(no_cores)
n <- length(data$x)
grilleh <- 1:200/1000
T1<-Sys.time()
hopt <- hCV(data$x,data$y)
n <- length(data$x)
grilleh <- 1:200/1000
T1<-Sys.time()
hopt <- hCV(data$x,data$y)
hCV <- function(X,Y,noy="G")
{
CV1 <- function(h)
{
CV(X,Y,h,noy="G")
#print(CV(X,Y,h,noy="G"))
}
#grilleh=1:200/1000
E <- sapply(grilleh,CV1)
#E <- mclapply(grilleh,CV1,mc.cores=no_cores)
return(list(h=grilleh[which.min(E)],erreur=E))
}
hopt <- hCV(data$x,data$y)
View(hopt)
T2<-Sys.time()
Tdiff <- difftime(T2, T1)
Tdiff
plot(grilleh,as.numeric(hopt$erreur))
y <- hopt$erreur[as.numeric(hopt$erreur)==min(as.numeric(hopt$erreur))]
abline(v=hopt$h,h=y,col="blue")
# Calcul de l'estimateur de Nadaraya-Watson pour chaque x et choix de h
# par la méthode de la validation croisée
h <- hopt$h
resu <-  rep(0,n)
for(i in 1: n)
{
print(i)
resu[i] <- NW(data$x[i],data$x,data$y,h)
}
plot(data$x,data$y)
curve(vraifonction,col="blue",add=T)
lines(data$x,resu,col="red")
View(data)
data$x
data
tau_g<-as.numeric(integrate(gauss(data$x)^2,-Inf,Inf)[1])
as.numeric(integrate(gauss2(data$x),-Inf,Inf)[1])
gauss2 <- function(x)
{
((1/(2*pi)^0.5)*exp(-x^2/2))^2
}
as.numeric(integrate(gauss2(data$x),-Inf,Inf)[1])
as.numeric(integrate(gauss2,-Inf,Inf)[1])
rectangle2 <- function(x)
{
(0.5*(abs(x)<=1))^2
}
triangle2 <- function(x)
{
((1-abs(x))*(abs(x)<=1))^2
}
gauss2 <- function(x)
{
((1/(2*pi)^0.5)*exp(-x^2/2))^2
}
epanechnikov2 <- function(x)
{
(((3/4)*(1-x^2))*(abs(x)<=1))^2
}
tau_g<-as.numeric(integrate(gauss2,-Inf,Inf)[1])
tau_g
tau_t<-as.numeric(integrate(triangle2,-Inf,Inf)[1])
tau_t
tau_r<-as.numeric(integrate(rectangle2,-Inf,Inf)[1])
tau_r
tau_e<-as.numeric(integrate(epanechnikov2,-Inf,Inf)[1])
tau_e
